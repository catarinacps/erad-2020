#+title: =qr-analysis= Lab-book
#+author: Henrique Silva
#+email: hcpsilva@inf.ufrgs.br
#+infojs_opt:
#+property: session *R*
#+property: cache yes
#+property: results graphics
#+property: exports both
#+property: tangle yes
#+seq_todo: TODO(t!) STARTED(s!) WAITING(w!) | DONE(d!) CANCELLED(c!) DEFERRED(f!)

Lab-book intended to document my efforts into extending Marcelo's
assignment/paper about QR Factorization analysis.

* Table of contents                                                   :TOC_3:
- [[#journal][Journal]]
  - [[#to-do][To-do]]
    - [[#active-items][Active items]]
    - [[#secondary-items][Secondary items]]
    - [[#archive][Archive]]
  - [[#daily][Daily]]
    - [[#2020-01-03][2020-01-03]]
  - [[#meetings][Meetings]]
- [[#experiments][Experiments]]
  - [[#preamble][Preamble]]
    - [[#launching-the-experiments][Launching the experiments]]
    - [[#installing-dependencies][Installing dependencies]]
  - [[#1---overview---makespan-only][1 - Overview - makespan only]]
    - [[#design][Design]]
    - [[#dependencies][Dependencies]]
    - [[#makefile][Makefile]]
    - [[#script][Script]]
  - [[#2---tracing][2 - Tracing]]
    - [[#design-1][Design]]
    - [[#dependencies-1][Dependencies]]
    - [[#makefile-1][Makefile]]
    - [[#script-1][Script]]
    - [[#post-processing][Post-processing]]
- [[#resources][Resources]]
  - [[#archive-1][Archive]]
    - [[#deps-script-backup][Deps script backup]]

* Journal
:PROPERTIES:
:ATTACH_DIR: attachments/
:END:

Intended to hold any meaningful advance yet not meaningful enough to deserve its
own topic in my lab-book.

** To-do

*** Active items

**** TODO Run the first experiment!
- State "TODO"       from              [2020-01-03 sex 14:15]

As its kinda time-critical right now.

*** Secondary items

Empty! :)

*** Archive

**** DONE Talk to Marcelo about the script
CLOSED: [2019-10-16 qua 17:31]
- State "DONE"       from "TODO"       [2019-10-16 qua 17:31]
- State "TODO"       from              [2019-10-05 s√°b 20:14]

** Daily

Daily thoughts and developments.

*** 2020-01-03

First day of this repo!

** Meetings

Like daily entries, but for meetings.

* Experiments

Hi! So, as I'll put into further details later, I use a script to run
experiments.

The so called "experiment IDs" are both sub-directories in the experiments
directory and tags in subsections of this section.

** Preamble

Important info if you are looking to reproduce my way of doing things.

Firstly though, I'd like you to read about how I run these experiments myself:

#+begin_src bash :exports both
cd <COMMON_DIRECTORY>/qr_analysis

./experiments/launch.sh --help
#+end_src

This command will return the following usage guide:

#+begin_example
COMMAND:
  $0 [OPTIONS] <EXP_ID> [REPO_DIRECTORY]

  WHERE <EXP_ID> is the identificator of the experiment

  WHERE [OPTIONS] can be any of the following, in no particular order:
    -h | --help
      shows this message and exits
    -d | --dry
      prints what it would do instead of actually doing
    -u | --update
      updates the repo before running any commands
    -i | --install=path/to/the/installs
      use another dir instead of the default $HOME/Installs/spack
    -l | --local
      install all packages locally in each machine
      WARNING: probably won't work because of timeouts in spack

  WHERE [REPO_DIRECTORY] is the *full* path to the repository
    It is presumed that you are in it, if you don't provide this argument
#+end_example

*** Launching the experiments

Hello! I suppose you've read the info in the previous section, so, here's the
deal: I use a script to run experiments.

Here's the maximum value of the elements in the matrices we'll deal with:

#+name: values_range
#+begin_src bash :results output :exports results
echo 100
#+end_src

#+RESULTS: values_range
: 100

It's big, I know, but it ensures that they run in the right nodes and it
installs all needed dependencies! Take a look at it:

#+begin_src bash :shebang "#!/bin/bash" :results none :tangle experiments/launch.sh
# more robust script
set -euo pipefail

function usage()
{
    echo "COMMAND:"
    echo "  $0 [OPTIONS] <EXP_ID> [REPO_DIRECTORY]"
    echo
    echo "  WHERE <EXP_ID> is the identificator of the experiment"
    echo
    echo "  WHERE [OPTIONS] can be any of the following, in no particular order:"
    echo "    -h | --help"
    echo "      shows this message and exits"
    echo "    -d | --dry"
    echo "      prints what it would do instead of actually doing it"
    echo "    -u | --update"
    echo "      updates the repo before running any commands"
    echo "    -i | --install[=]path/to/the/installs"
    echo "      use another dir instead of the default $HOME/Installs/spack"
    echo "    -p | --partitions[=]list,of,partitions,comma,separated"
    echo "      define the desired partitions to be used (default: cei)"
    echo "    -n | --nodes[=]list,of,nodes,comma,separated"
    echo "      define the desired nodes to be used"
    echo "      WARNING: this option disables usage of the partition list!"
    echo "    -l | --local"
    echo "      install all packages locally in each machine"
    echo "      WARNING: probably won't work because of timeouts in spack"
    echo
    echo "  WHERE [REPO_DIRECTORY] is the *full* path to the repository"
    echo "    It is presumed that you are in it, if you don't provide this argument"
}

for i in "$@"; do
    case $i in
        -h|--help)
            echo "USAGE:"
            echo
            usage
            exit
            ;;
        -d|--dry)
            DRY=true
            shift
            ;;
        -u|--update)
            UPDATE=true
            shift
            ;;
        --install=*)
            INSTALL_DIR=${i#*=}
            shift
            ;;
        -i|--install)
            shift
            INSTALL_DIR=$1
            shift
            ;;
        --partitions=*)
            PARTITIONLIST=$(tr ',' ' ' <<<${i#*=})
            shift
            ;;
        -p|--partitions)
            shift
            PARTITIONLIST=$(tr ',' ' ' <<<$1)
            shift
            ;;
        --nodes=*)
            NODELIST=$(tr ',' '\n' <<<${i#*=})
            PARTITIONLIST=$(sed -E 's/([0-9]+)//g' <<<$NODELIST | uniq | xargs)
            shift
            ;;
        -n|--nodes)
            shift
            NODELIST=$(tr ',' '\n' <<<$1)
            PARTITIONLIST=$(sed -E 's/([0-9]+)//g' <<<$NODELIST | uniq | xargs)
            shift
            ;;
        -l|--local)
            INSTALL_DIR=/scratch/$USER/installs
            LOCAL=true
            shift
            ;;
    esac
done

# directory with needed dependencies installed
INSTALL_DIR=${INSTALL_DIR:-$HOME/Installs/spack}

# the experiment id
EXPERIMENT_ID=$1

# the work (repo) dir
REPO_DIR=${2:-$(pwd)}

# dry run boolean
DRY=${DRY:-false}

# default run partition
PARTITIONLIST=${PARTITIONLIST:-cei}

# update boolean
UPDATE=${UPDATE:-false}

# local install boolean
LOCAL=${LOCAL:-false}

if [[ $REPO_DIR != /* ]]; then
    echo "Path to repository is not absolute, please use the absolute path..."
    exit
fi

if [[ $INSTALL_DIR != /* ]]; then
    echo "Path to installation dir is not absolute, please use the absolute path..."
    exit
fi

EXP_DIR=$(find $REPO_DIR -type d -path "*/experiments/$EXPERIMENT_ID")
if [ ! -n "$EXP_DIR" ]; then
    echo "There isn't any experiment with this ID..."
    exit
fi

pushd $REPO_DIR

# update the repo?
[ $UPDATE = true ] && git pull

for partition in $PARTITIONLIST; do
    # lets install all needed dependencies first
    echo "Launching dependency installing job for partition $partition!"
    if [ $DRY = true -a $LOCAL != true ]; then
        echo "sbatch"
        echo "-p $partition"
        echo "-N 1"
        echo "-J dependencies_${EXPERIMENT_ID}_${partition}"
        echo "-W"
        echo "$(dirname $EXP_DIR)/deps.sh $INSTALL_DIR $EXP_DIR"
        echo
    elif [ $LOCAL != true ]; then
        sbatch \
            -p ${partition} \
            -N 1 \
            -J dependencies_${EXPERIMENT_ID}_${partition} \
            -W \
            $(dirname $EXP_DIR)/deps.sh $INSTALL_DIR $EXP_DIR
    fi
    echo "... and done!"
    echo

    # change the gppd-info to sinfo when porting
    ALLNODES=$(gppd-info --long --Node -S NODELIST -p $partition -h | awk '{print $1 "_" $5}')
    if [ -z ${NODELIST+x} ]; then
        nodes=$(paste -s -d" " - <<<$ALLNODES)
    else
        nodes=$(grep "$NODELIST" <<<$ALLNODES | paste -s -d" " -)
    fi

    for execution in $nodes; do
        # launch the slurm script for this node
        echo "Launching job for node ${execution%%_*}..."
        if [ $DRY = true ]; then
            echo "sbatch"
            echo "-p ${partition}"
            echo "-w ${execution%%_*}"
            echo "-c ${execution#*_}"
            echo "-J qr_analysis_${EXPERIMENT_ID}"
            echo "$EXP_DIR/exp.slurm $EXPERIMENT_ID $EXP_DIR $INSTALL_DIR $LOCAL"
            echo
        else
            sbatch \
                -p ${partition} \
                -w ${execution%%_*} \
                -c ${execution#*_} \
                -J qr_analysis_${EXPERIMENT_ID} \
                $EXP_DIR/exp.slurm $EXPERIMENT_ID $EXP_DIR $INSTALL_DIR $LOCAL
        fi
    done
done

popd
#+end_src

*** Installing dependencies

Here shall lie the automatic dependencies installer...

#+begin_src bash :shebang "#!/bin/bash" :results none :tangle experiments/deps.sh
#SBATCH --time=3:00:00
#SBATCH --chdir=.
#SBATCH --output=/home/users/hcpsilva/slurm_outputs/%x_%j.out
#SBATCH --error=/home/users/hcpsilva/slurm_outputs/%x_%j.err
#SBATCH --mail-type=END,FAIL
#SBATCH --mail-user=hcpsilva@inf.ufrgs.br

# more robust script
set -euo pipefail

# to install spack dependencies
function spack_install_spec {
    SPEC=$1
    ARCH=$2

    name_version=${SPEC%%[~|+|^]*}
    dir_name=$(echo $name_version | tr '@' '-')

    # if we fall here, we have already installed the package
    [ -d $dir_name ] && return 0

    echo "${name_version} not yet installed!"
    mkdir $dir_name
    spack install --keep-stage $SPEC arch=$ARCH
    spack view -d true soft -i $dir_name $SPEC arch=$ARCH

    [ ! -f installs.log ] && echo "SPECS HERE INSTALLED" > installs.log
    echo >> installs.log
    echo "PACKAGE:\t${name_version}" >> installs.log
    echo "SPEC:\t${SPEC}" >> installs.log
}

INSTALL_DIR=$1/$SLURM_JOB_PARTITION
EXP_DIR=$2
SPACK_DIR=${3:-$HOME/spack-erad}

pushd $HOME

if [ ! -d $SPACK_DIR ]; then
    echo "spack not yet installed!"
    git clone http://gitlab+deploy-token-127235:BZMob8RJoRPZAdLtsstX@gitlab.com/viniciusvgp/customSpack.git $SPACK_DIR
    pushd $SPACK_DIR
    ./install_spack.sh -symr
    popd
fi

. $SPACK_DIR/src/spack/share/spack/setup-env.sh

# find available compilers for this machine
spack compiler find

# get current node info
arch=$(spack arch)

# create the install dir if there isn't one
[ ! -d $INSTALL_DIR ] && mkdir -p $INSTALL_DIR

pushd $INSTALL_DIR

echo "--> INSTALLING DEPENDENCIES"

while read -r method spec; do
    echo $method $spec

    case $method in
        spack)
            spack_install_spec $spec $arch
            ;;
        manual)
            $EXP_DIR/${spec//@/-}.sh $INSTALL_DIR
            ;;
        ,*)
            echo
            echo "ERROR: method not supported..."
            exit
            ;;
    esac
done < $EXP_DIR/exp.deps

echo
echo "--> DONE"

popd
popd
#+end_src

** WAITING 1 - Overview - makespan only                              :EXP01:
- State "WAITING"    from "STARTED"    [2020-01-09 qui 15:22]
- State "STARTED"    from              [2020-01-09 qui 15:22]

Only a makespan analysis of all different runtime options, no tracing involved

*** Design

The random seed will be:

#+begin_src R :session :results value :exports results
floor(runif(1,1,99999))
#+end_src

#+RESULTS:
: 86229

Finally, the design itself:

#+begin_src R :session :results output :var expKey="exp01"
suppressMessages(library(tidyverse))
suppressMessages(library(DoE.base))

matrix = c(1024, 2048, 4096, 8192, 16384, 32768)
nb = c(32, 64, 128, 256, 512)
method = c("starpu", "libomp", "libgomp", "libkomp_gcc", "libkomp_clang", "kstar_starpu")

complete <- fac.design(
  nfactors=3,
  replications=3,
  repeat.only=FALSE,
  blocks=1,
  randomize=TRUE,
  seed=86229,
  factor.names=list(
    matrix_size=matrix,
    block_size=nb,
    runtime=method)) %>%
  as_tibble %>%
  filter(matrix_size == 8192) %>%
  transmute(id=as.numeric(Blocks), runtime, matrix_size, block_size) %>%
  write_delim(paste0("experiments/", expKey, "/runs.plan"), delim=" ", col_names=FALSE)

# the space delimited file is to help with the posterior parsing in the shell
# script
#+end_src

#+RESULTS:
:
: creating full factorial with 180 runs ...

*** Dependencies

In this experiment we'll need the following =spack= packages:

- starpu-1.3
- llvm
- netlib-lapack
- libkomp
- kstar-starpu

#+begin_src text :exports both :tangle experiments/exp01/exp.deps
spack openblas@0.3.7
spack hdf5@1.10.5
spack starpu@1.3.1~fxt~poti~examples~mpi+openmp
spack netlib-lapack@3.8.0
manual libomp@6.0
spack libkomp@master+the+affinity+numa~tracing~papi+vardep
spack kstar@master+starpu^starpu@1.3.1~fxt~poti~examples~mpi+openmp
#+end_src

**** libomp-6.0

From the [[https://github.com/llvm-mirror/openmp][LLVM stdlib]]:

#+begin_src bash :shebang "#!/bin/bash" :exports both :results none :tangle experiments/exp01/libomp-6.0.sh
set -euo pipefail

INSTALL_DIR=$1
LIBOMP_DIR=$INSTALL_DIR/libomp-6.0

[ -d $LIBOMP_DIR ] && exit

echo "libomp not yet installed!"
pip install --user lit
mkdir $LIBOMP_DIR
git clone https://github.com/llvm-mirror/openmp.git $LIBOMP_DIR/openmp
pushd $LIBOMP_DIR/openmp
git checkout release_60
mkdir build
pushd build
LLVM_PATHS=$(find /usr/lib -name 'llvm-[0-9]*' | sed -e 's/$/\/bin/' | paste -s -d':' -)
export PATH+=:$LLVM_PATHS
cmake \
    -DCMAKE_C_COMPILER=clang \
    -DCMAKE_CXX_COMPILER=clang++ \
    -DCMAKE_INSTALL_PREFIX=$LIBOMP_DIR \
    -DLIBOMP_OMPT_SUPPORT=on \
    -DLIBOMP_OMPT_OPTIONAL=on \
    -DLIBOMP_STATS=on \
    ..
make -j
make -j install
popd
popd
#+end_src

*** Makefile

The =makefile= used in this experiment!

#+begin_src makefile :tangle experiments/exp01/code/Makefile
OBJ_DIR := bin
OUT_DIR := build
SRC_DIR := src
LIB_DIR := lib
INC_DIR := include

DEBUG :=

LIB_EXTRA :=
INC_EXTRA :=

#	- Compilation flags:
#	Compiler and language version
CC := gcc
KSTAR := kstar --runtime starpu
DEBUGF := $(if $(DEBUG),-g -fsanitize=address)
CFLAGS :=\
	-Wall \
	-Wextra \
	-Wpedantic \
	-Wshadow \
	-Wunreachable-code
OMP := -fopenmp
OPT := $(if $(DEBUG),-O0,-O2 -march=native)
LIB := -L$(LIB_DIR) $(LIB_EXTRA)\
	$(shell pkg-config lapack lapacke blas starpu-1.3 hwloc --libs)\
	-lm
INC := -I$(INC_DIR) -I$(SRC_DIR) $(INC_EXTRA)\
	$(shell pkg-config lapack lapacke blas starpu-1.3 --cflags)

#	Should be defined in the command line
LIBOMP_LIB :=
LIBOMP_INC :=
LIBOMP := -L$(LIBOMP_LIB) -Wl,--rpath,$(LIBOMP_LIB) -I$(LIBOMP_INC)

################################################################################
#	Files:

#	- Path to all final binaries:
TARGET := $(OUT_DIR)/block_qr_libgomp $(OUT_DIR)/block_qr_starpu $(OUT_DIR)/block_qr_libomp $(OUT_DIR)/matrix_generator $(OUT_DIR)/block_qr_kstar_starpu

################################################################################
#	Targets:

.DEFAULT_GOAL = all

all: $(TARGET)

#
# mutils
#
$(OBJ_DIR)/mutils.o: $(SRC_DIR)/mutils/mutils.c
	$(CC) -c -o $@ $^ $(INC) $(CFLAGS)

$(OBJ_DIR)/mutils_kstar.o: $(SRC_DIR)/mutils/mutils.c
	$(KSTAR) -c -o $@ $^ $(INC) $(CFLAGS)

#
# OPENMP task based parallel blocked QR factorization
#
$(OBJ_DIR)/block_qr_libgomp.o: $(SRC_DIR)/block_qr_openmp.c
	$(CC) -c -o $@ $^ $(INC) $(OMP) $(CFLAGS)

$(OUT_DIR)/block_qr_libgomp: $(OBJ_DIR)/block_qr_libgomp.o $(OBJ_DIR)/mutils.o
	$(CC) -o $@ $^ $(OMP) $(LIB)

#
# STARPU task based parallel blocked QR factorization
#
$(OBJ_DIR)/block_qr_starpu.o: $(SRC_DIR)/block_qr_starpu.c
	$(CC) -c -o $@ $^ $(INC) $(CFLAGS)

$(OUT_DIR)/block_qr_starpu: $(OBJ_DIR)/block_qr_starpu.o $(OBJ_DIR)/mutils.o
	$(CC) -o $@ $^ $(INC) $(LIB)


#
# OpenMP with llvm runtime (libomp)
#
$(OBJ_DIR)/block_qr_libomp.o: $(SRC_DIR)/block_qr_openmp.c
	$(CC) -c -o $@ $^ $(INC) $(OMP) $(LIBOMP) $(CFLAGS)

$(OUT_DIR)/block_qr_libomp: $(OBJ_DIR)/block_qr_libomp.o $(OBJ_DIR)/mutils.o
	$(CC) -o $@ $^ $(LIB) $(OMP) $(LIBOMP)

#
# Kstar with starpu runtime
#
$(OBJ_DIR)/block_qr_kstar_starpu.o: $(SRC_DIR)/block_qr_openmp.c
	$(KSTAR) -c -o $@ $^ $(INC) $(OMP) $(LIBOMP) $(CFLAGS)

$(OUT_DIR)/block_qr_kstar_starpu: $(OBJ_DIR)/block_qr_kstar_starpu.o $(OBJ_DIR)/mutils_kstar.o
	$(KSTAR) -o $@ $^ $(LIB) $(OMP) $(LIBOMP)

#
# Matrix Generator
#
$(OUT_DIR)/matrix_generator: $(SRC_DIR)/matrix_generator.c
	$(CC) -o $@ $^ $(CFLAGS)

# misc

print-%:
	@echo $* = $($*)

clean:
	rm -f $(OBJ_DIR)/*.o $(INC_DIR)/*~ $(TARGET) $(LIB_DIR)/*.so *~ *.o
#+end_src

*** Script

I'm using my script as a base because his script is, well, not pretty.

#+begin_src bash :shebang "#!/bin/bash" :tangle experiments/exp01/exp.slurm
#SBATCH --time=24:00:00
#SBATCH --chdir=.
#SBATCH --output=/home/users/hcpsilva/slurm_outputs/%x_%j.out
#SBATCH --error=/home/users/hcpsilva/slurm_outputs/%x_%j.err
#SBATCH --mail-type=END,FAIL
#SBATCH --mail-user=hcpsilva@inf.ufrgs.br

# more robust script
# set -euo pipefail

# parameters:
# the experiment ID, defined in the lab-book
EXP_ID=$1
# the experiment directory
EXP_DIR=$2
# the path to the directory where we'll find the needed packages
INSTALL=$3/$SLURM_JOB_PARTITION
# local run?
LOCAL=$4

# node name
HOST=$(hostname)

# maximum element value (defined in experiment design)
MAXVAL=100

# experiment name (which is the ID and the machine and its core count)
EXP_NAME=${EXP_ID}_${HOST}_${SLURM_CPUS_ON_NODE}

# seed generated in project design
RAND_SEED=86229

# go to the scratch dir to execute our operations
cd $SCRATCH

# clean up my scratch dir
rm -rf *

# if the LOCAL argument is true, install everything locally
# (we presume that the path is $INSTALL is local)
[ $LOCAL = true ] && $EXP_DIR/deps.sh $INSTALL ./spack

STARPU_PATH=$(readlink -f $INSTALL/starpu-1.3.1)
LIBOMP_PATH=$(readlink -f $INSTALL/libomp-6.0)
LAPACK_PATH=$(readlink -f $INSTALL/netlib-lapack-3.8.0)
HDF5_PATH=$(readlink -f $INSTALL/hdf5-1.10.5)
OPENBLAS_PATH=$(readlink -f $INSTALL/openblas-0.3.7)
LIBKOMP_PATH=$(readlink -f $INSTALL/libkomp-master)
KSTAR_PATH=$(readlink -f $INSTALL/kstar-master)

PATH+=:$STARPU_PATH/bin
PATH+=:$KSTAR_PATH/bin
export PATH=$PATH

PKG_CONFIG_PATH+=:$STARPU_PATH/lib/pkgconfig
PKG_CONFIG_PATH+=:$LAPACK_PATH/lib/pkgconfig
export PKG_CONFIG_PATH=$PKG_CONFIG_PATH

# prepare env variables
threads_per_core=$(lscpu | grep "per core" | awk '{print $4}')
real_core_count=$((${SLURM_CPUS_ON_NODE} / ${threads_per_core:-1}))
export STARPU_NCPU=$real_core_count
export OMP_NUM_THREADS=$real_core_count
export STARPU_FXT_TRACE=0
export KAAPI_RECORD_TRACE=0
export OMP_PLACES={0:$real_core_count}
export OMP_PROC_BIND=true
export KMP_STACKSIZE=$((1024*1024*34))

echo "Environment variables set up!"

# prepare our directory
mkdir $EXP_NAME
pushd $EXP_NAME

# copy the code folder
cp -r $EXP_DIR/code code
mkdir results

pushd code
make clean
make all LIBOMP_LIB="$LIBOMP_PATH/lib" LIBOMP_INC="$LIBOMP_PATH/include"
ln -s build/block_qr_libomp build/block_qr_libkomp_clang
ln -s build/block_qr_libgomp build/block_qr_libkomp_gcc
popd

# init the results csv
results_csv=results/${HOST}_data.csv
echo "node,rep_id,matrix_size,block_size,runtime,compute_time,total_time" > $results_csv

# execute the experiment
while read -r id runtime matrix num_blocks; do
    echo "-> Parameters set to: $runtime $matrix $num_blocks"

    # output log file
    log_file=results/${runtime}_${matrix}_${num_blocks}_${id}.log

    # execute given runtime and log results

    LD_LIBRARY_PATH=$LAPACK_PATH/lib

    if [[ $runtime = starpu ]] || [[ $runtime = kstar_starpu ]]; then
        LD_LIBRARY_PATH+=:$HDF5_PATH/lib
        LD_LIBRARY_PATH+=:$OPENBLAS_PATH/lib
        LD_LIBRARY_PATH+=:$STARPU_PATH/lib
    elif [[ $runtime = openmp ]]; then
        LD_LIBRARY_PATH+=:$LIBOMP_PATH/lib
    elif [[ $runtime = libkomp_gcc ]] || [[ $runtime = libkomp_clang ]]; then
        LD_LIBRARY_PATH+=:$LIBKOMP_PATH/lib
    fi

    export LD_LIBRARY_PATH=$LD_LIBRARY_PATH

    ./code/build/block_qr_$runtime \
                   $matrix \
                   $num_blocks \
                   $RAND_SEED \
                   $MAXVAL > $log_file 2>&1

    # get compute and total times from output
    ctime=$(grep -w compute_time $log_file | awk '{print $2}')
    ttime=$(grep -w total_time $log_file | awk '{print $2}')

    # add the execution data to the csv
    echo ${HOST},${id},${matrix},${num_blocks},${runtime},${ctime},${ttime} >> $results_csv

    echo
done < $EXP_DIR/runs.plan

# gather node info
./code/scripts/node_info.sh > env.node

# create the data dir if it isn't already there
[ ! -d $EXP_DIR/data ] && mkdir $EXP_DIR/data

# zip everything and commit to EXP_DIR
tar czf $EXP_DIR/data/${EXP_NAME}_data.tar.gz *

popd
rm -rf $SCRATCH/*
#+end_src

** STARTED 2 - Tracing
- State "STARTED"    from              [2020-01-09 qui 15:22]

Following the idea behind the third experiment, this experiment is a reduced
version of experiment 02! I'll use the same matrix size as the third experiment,
=8192=.

*** Design

Again we are going with an exhaustive design until second notice.

The random seed will be:

#+begin_src R :session :results value :exports results
floor(runif(1,1,99999))
#+end_src

#+RESULTS:
: 15195

And the design:

#+begin_src R :session :results none
suppressMessages(library(tidyverse))
suppressMessages(library(DoE.base))

matrix = c(1024, 2048, 4096, 8192, 16384, 32768)
nb = c(32, 64, 128, 256, 512)
method = c("starpu", "ompt", "scorep", "libkomp_gcc", "libkomp_clang", "kstar_starpu")

fac.design(
  nfactors=3,
  replications=1,
  repeat.only=FALSE,
  blocks=1,
  randomize=TRUE,
  seed=15195,
  factor.names=list(
    matrix_size=matrix,
    block_size=nb,
    runtime=method)) %>%
  as_tibble %>%
  filter(matrix_size == 8192) %>%
  transmute(runtime, matrix_size, block_size) %>%
  write_delim("experiments/exp02/runs.plan", delim=" ", col_names=FALSE)

# the space delimited file is to help with the posterior parsing in the shell
# script
#+end_src

*** Dependencies

In this experiment we'll need the following =spack= packages:

- starpu-1.3
- llvm
- netlib-lapack
- scorep

#+begin_src bash :tangle experiments/exp02/exp.deps
spack openblas@0.3.7
spack hdf5@1.10.5
spack starpu@1.3.1+fxt+poti~examples~mpi+openmp
spack netlib-lapack@3.8.0
manual libomp@6.0
spack libkomp@master+the+affinity+numa+tracing+papi+vardep
spack kstar@master+starpu^starpu@1.3.1+fxt+poti~examples~mpi+openmp
spack scorep@6.0
#+end_src

**** libomp-6.0

From the [[https://github.com/llvm-mirror/openmp][LLVM stdlib]]:

#+begin_src bash :shebang "#!/bin/bash" :exports both :results none :tangle experiments/exp02/libomp-6.0.sh
set -euo pipefail

INSTALL_DIR=$1
LIBOMP_DIR=$INSTALL_DIR/libomp-6.0

[ -d $LIBOMP_DIR ] && exit

echo "libomp not yet installed!"
pip install --user lit
mkdir $LIBOMP_DIR
git clone https://github.com/llvm-mirror/openmp.git $LIBOMP_DIR/openmp
pushd $LIBOMP_DIR/openmp
git checkout release_60
mkdir build
pushd build
LLVM_PATHS=$(find /usr/lib -name 'llvm-[0-9]*' | sed -e 's/$/\/bin/' | paste -s -d':' -)
export PATH+=:$LLVM_PATHS
cmake \
    -DCMAKE_C_COMPILER=clang \
    -DCMAKE_CXX_COMPILER=clang++ \
    -DCMAKE_INSTALL_PREFIX=$LIBOMP_DIR \
    -DLIBOMP_OMPT_SUPPORT=on \
    -DLIBOMP_OMPT_OPTIONAL=on \
    -DLIBOMP_STATS=on \
    ..
make -j
make -j install
popd
popd
#+end_src

*** Makefile

The =makefile= used in this experiment!

#+begin_src makefile :tangle experiments/exp02/code/Makefile
OBJ_DIR := bin
OUT_DIR := build
SRC_DIR := src
LIB_DIR := lib
INC_DIR := include

DEBUG :=

#	- Compilation flags:
#	Compiler and language version
CC := gcc
PRELINK := scorep
DEBUGF := $(if $(DEBUG),-g -fsanitize=address)
CFLAGS :=\
	-Wall \
	-Wextra \
	-Wpedantic \
	-Wshadow \
	-Wunreachable-code
OMP := -fopenmp
OPT := $(if $(DEBUG),-O0,-O2 -march=native)
LIB := -L$(LIB_DIR) \
	$(shell pkg-config lapack lapacke blas starpu-1.3 fxt poti hwloc --libs)\
	-lm
INC := -I$(INC_DIR) -I$(SRC_DIR) \
	$(shell pkg-config lapack lapacke blas starpu-1.3 --cflags)

# LIBOMP := -L$(LIBOMP_LIB) -Wl,--rpath,$(LIBOMP_LIB) -I$(LIBOMP_INC)

#	Should be defined in the command line
LIBOMP_LIB :=
LIBOMP_INC :=
LIBOMP := -L$(LIBOMP_LIB) -Wl,--rpath,$(LIBOMP_LIB) -I$(LIBOMP_INC)

################################################################################
#	Files:

# 	- List of targets
TARGET_EXE := $(OUT_DIR)/block_qr_scorep $(OUT_DIR)/block_qr_starpu $(OUT_DIR)/block_qr_ompt $(OUT_DIR)/matrix_generator $(OUT_DIR)/block_qr_kstar_starpu

#	- Path to all final libraries:
TARGET_LIB := $(patsubst %, $(LIB_DIR)/lib%.so, $(shell basename $(shell find $(LIB_DIR)/* -maxdepth 0 -type d)))

################################################################################
#	Targets:

.DEFAULT_GOAL = all

all: $(TARGET_LIB) $(TARGET_EXE)

#
# mutils
#
$(OBJ_DIR)/mutils.o: $(SRC_DIR)/mutils/mutils.c
	$(CC) -c -o $@ $^ $(INC) $(CFLAGS)

$(OBJ_DIR)/mutils_kstar.o: $(SRC_DIR)/mutils/mutils.c
	$(KSTAR) -c -o $@ $^ $(INC) $(CFLAGS)

#
# SCOREP - OPENMP task based parallel blocked QR factorization
#
$(OUT_DIR)/block_qr_scorep: $(SRC_DIR)/block_qr_openmp.c $(OBJ_DIR)/mutils.o
	$(PRELINK) $(CC) -o $@ $^ $(INC) $(OMP) $(LIB) $(CFLAGS)

#
# STARPU task based parallel blocked QR factorization
#
$(OUT_DIR)/block_qr_starpu: $(SRC_DIR)/block_qr_starpu.c $(OBJ_DIR)/mutils.o
	$(CC) -o $@ $^ $(INC) $(LIB) $(CFLAGS)

#
# Lib for getting OMPT traces
#
$(LIB_DIR)/libinit.so: $(LIB_DIR)/init/initialization.c $(LIB_DIR)/init/initialization.h
	$(CC) $^ -o $@ -shared -fPIC $(CFLAGS) $(LIBOMP) $(OMP)

$(OUT_DIR)/block_qr_ompt: $(SRC_DIR)/block_qr_ompt.c $(OBJ_DIR)/mutils.o
	$(CC) -o $@ $^ $(INC) $(LIB) $(CFLAGS) $(OMP) $(LIBOMP) -DDYN_TOOL -linit

#
# Kstar with starpu runtime
#
$(OBJ_DIR)/block_qr_kstar_starpu.o: $(SRC_DIR)/block_qr_openmp.c
	$(KSTAR) -c -o $@ $^ $(INC) $(OMP) $(LIBOMP) $(CFLAGS)

$(OUT_DIR)/block_qr_kstar_starpu: $(OBJ_DIR)/block_qr_kstar_starpu.o $(OBJ_DIR)/mutils_kstar.o
	$(KSTAR) -o $@ $^ $(LIB) $(OMP) $(LIBOMP)

#
# Matrix Generator
#
$(OUT_DIR)/matrix_generator: $(SRC_DIR)/matrix_generator.c
	$(CC) -o $@ $^ $(CFLAGS)

print-%:
	@echo "$* == $($*)"

clean:
	rm -f $(OBJ_DIR)/*.o $(INC_DIR)/*~ $(OUT_DIR)/* $(TARGET_EXE) $(LIB_DIR)/*.so *~ *.o
#+end_src

*** Script

It's going to be based off the last script...

#+begin_src bash :shebang "#!/bin/bash" :tangle experiments/exp02/exp.slurm
#SBATCH --time=24:00:00
#SBATCH --chdir=.
#SBATCH --output=/home/users/hcpsilva/slurm_outputs/%x_%j.out
#SBATCH --error=/home/users/hcpsilva/slurm_outputs/%x_%j.err
#SBATCH --mail-type=END,FAIL
#SBATCH --mail-user=hcpsilva@inf.ufrgs.br

# more robust script
set -euo pipefail

# parameters:
# the experiment ID, defined in the lab-book
EXP_ID=$1
# the experiment directory
EXP_DIR=$2
# the path to the directory where we'll find the needed packages
INSTALL=$3/$SLURM_JOB_PARTITION
# local run?
LOCAL=$4

# node name
HOST=$(hostname)

# maximum element value (defined in experiment design)
MAXVAL=100

# experiment name (which is the ID and the machine and its core count)
EXP_NAME=${EXP_ID}_${HOST}_${SLURM_CPUS_ON_NODE}

# seed generated in project design
RAND_SEED=15195

# go to the scratch dir to execute our operations
cd $SCRATCH

# clean up my scratch dir
rm -rf *

# if the LOCAL argument is true, install everything locally
# (we presume that the path is $INSTALL is local)
[ $LOCAL = true ] && $EXP_DIR/deps.sh $INSTALL ./spack

STARPU_PATH=$(readlink -f $INSTALL/starpu-1.3.1)
LIBOMP_PATH=$(readlink -f $INSTALL/libomp-6.0)
LAPACK_PATH=$(readlink -f $INSTALL/netlib-lapack-3.8.0)
SCOREP_PATH=$(readlink -f $INSTALL/scorep-6.0)
HDF5_PATH=$(readlink -f $INSTALL/hdf5-1.10.5)
OPENBLAS_PATH=$(readlink -f $INSTALL/openblas-0.3.7)
LIBKOMP_PATH=$(readlink -f $INSTALL/libkomp-master)
KSTAR_PATH=$(readlink -f $INSTALL/kstar-master)

PATH+=:$STARPU_PATH/bin
PATH+=:$SCOREP_PATH/bin
export PATH=$PATH

PKG_CONFIG_PATH+=:$STARPU_PATH/lib/pkgconfig
PKG_CONFIG_PATH+=:$LAPACK_PATH/lib/pkgconfig
export PKG_CONFIG_PATH=$PKG_CONFIG_PATH

# prepare env variables
threads_per_core=$(lscpu | grep "per core" | awk '{print $4}')
real_core_count=$((${SLURM_CPUS_ON_NODE} / ${threads_per_core:-1}))
export STARPU_NCPU=$real_core_count
export OMP_NUM_THREADS=$real_core_count
export OMP_PLACES={0:$real_core_count}
export OMP_PROC_BIND=true

export SCOREP_ENABLE_PROFILING=true
export SCOREP_ENABLE_TRACING=true
export SCOREP_VERBOSE=true
export SCOREP_TIMER=gettimeofday
# why 8G?
export SCOREP_TOTAL_MEMORY=8G

echo "Environment variables set up!"

# prepare our directory
mkdir $EXP_NAME
pushd $EXP_NAME

# copy the code folder
cp -r $EXP_DIR/code code
mkdir results

mkdir results/libgomp
mkdir results/starpu
mkdir results/libomp

LIBGOMP_TRACE_DIR=$(readlink -f results)/libgomp
STARPU_TRACE_DIR=$(readlink -f results)/starpu
LIBOMP_TRACE_DIR=$(readlink -f results)/libomp

export STARPU_FXT_TRACE=1

export SCOREP_EXPERIMENT_DIRECTORY=$LIBGOMP_TRACE_DIR

pushd code
make clean
make all LIBOMP_LIB="$LIBOMP_PATH/lib" LIBOMP_INC="$LIBOMP_PATH/include"
ln -s build/block_qr_libomp build/block_qr_libkomp_clang
ln -s build/block_qr_libgomp build/block_qr_libkomp_gcc
popd

# init the results csv
results_csv=results/${HOST}_data.csv
echo "node,matrix_size,block_size,runtime,compute_time,total_time" > $results_csv

# execute the experiment
while read -r runtime matrix num_blocks; do
    echo "-> Parameters set to: $runtime $matrix $num_blocks"
    run_id=${runtime}_${matrix}_${num_blocks}

    # output log file
    log_file=results/${run_id}.log

    LD_LIBRARY_PATH=$(readlink -f code/lib):$LAPACK_PATH/lib

    if [[ $runtime = starpu ]] || [[ $runtime = kstar_starpu ]]; then
        LD_LIBRARY_PATH+=:$HDF5_PATH/lib
        LD_LIBRARY_PATH+=:$OPENBLAS_PATH/lib
        LD_LIBRARY_PATH+=:$STARPU_PATH/lib
    elif [[ $runtime = openmp ]]; then
        LD_LIBRARY_PATH+=:$LIBOMP_PATH/lib
    elif [[ $runtime = libkomp_gcc ]] || [[ $runtime = libkomp_clang ]]; then
        LD_LIBRARY_PATH+=:$LIBKOMP_PATH/lib
    fi

    export LD_LIBRARY_PATH=$LD_LIBRARY_PATH

    # execute given runtime and log results
    ./code/build/block_qr_$runtime \
         $matrix \
         $num_blocks \
         $RAND_SEED \
         $MAXVAL > $log_file 2>&1

    # get compute and total times from output
    ctime=$(grep -w compute_time $log_file | awk '{print $2}')
    ttime=$(grep -w total_time $log_file | awk '{print $2}')

    case $runtime in
        starpu)
            mv /tmp/prof_file_${USER}_0 $STARPU_TRACE_DIR/${run_id}.trace
            ;;
        scorep)
            pid=0
            for file in $(find . -maxdepth 1 -name 'scorep-prof*'); do
                mv $file $LIBGOMP_TRACE_DIR/${run_id}_${pid}.trace
                pid=$((pid+1))
            done
            ;;
        ompt)
            mv events.out $LIBOMP_TRACE_DIR/${run_id}.trace
            ;;
        libkomp_gcc|libkomp_clang)
            echo
            ;;
        kstar_starpu)
            echo
            ;;
    esac

    # add the execution data to the csv
    echo ${HOST},${matrix},${num_blocks},${runtime},${ctime},${ttime} >> $results_csv

    echo
done < $EXP_DIR/runs.plan

# gather node info
./code/scripts/node_info.sh > env.node

# create the data dir if it isn't already there
[ ! -d $EXP_DIR/data ] && mkdir $EXP_DIR/data

# zip everything and commit to EXP_DIR
tar czf $EXP_DIR/data/${EXP_NAME}_data.tar.gz *

popd
rm -rf $SCRATCH/*
#+end_src

*** Post-processing

Now we gotta do some post-processing in order to manipulate all collected data.

#+begin_src bash :exports both :results output
set -euo pipefail

DATA_DIR=experiments/exp02/data

for tar_file in $DATA_DIR/*.tar.gz; do
    echo "Extracting ${tar_file}..."
    mkdir ${tar_file%.tar.gz}
    tar xzf $tar_file -C ${tar_file%.tar.gz}
done

echo "Done extracting the files!"
#+end_src

#+RESULTS:
: Extracting experiments/exp04/data/exp04_cei1_48_data.tar.gz...
: Extracting experiments/exp04/data/exp04_cei2_48_data.tar.gz...
: Extracting experiments/exp04/data/exp04_cei3_48_data.tar.gz...
: Extracting experiments/exp04/data/exp04_cei4_48_data.tar.gz...
: Extracting experiments/exp04/data/exp04_cei5_48_data.tar.gz...
: Extracting experiments/exp04/data/exp04_cei6_48_data.tar.gz...
: Extracting experiments/exp04/data/exp04_cei7_48_data.tar.gz...
: Extracting experiments/exp04/data/exp04_cei8_48_data.tar.gz...
: Done extracting the files!

With all files extracted, lets collect the traces and transform them into
CSVs...

**** libomp -> csv

A simple R function using =tidyr= will work here:

#+begin_src R :session :results output :exports both
options(crayon.enabled = FALSE)
supressMessages(library(tidyverse))
supressMessages(library(functional))

libomp_csv <- function(trace_file, out_file) {
  trace_file %>%
    read_delim(delim = " ", col_names = FALSE, col_types = cols()) %>%
    rename(thread = X1,
           task = X2,
           callback = X3,
           operation = X4,
           time = X5) %>%
    write_csv(out_file)
}

data_dir <- "experiments/exp04/data"

data_dir %>%
  list.files(pattern = glob2rx("ompt*.trace"),
             recursive = TRUE, full.names = TRUE) %>%
  mapply(., FUN = libomp_csv, lapply(paste, ., ".csv", sep = ""))
#+end_src

**** libgomp -> csv

Here we'll need an extra script to do the conversion, called
=otf2ompprint2paje.pl=:

#+begin_src bash :exports both :results output
# attention!
# you need both scorep and pajeng installed in order to run this command
# also, be sure that both are in your path

cd experiments/exp04/data

for trace_file in $(find . -name 'scorep*.trace'); do
    ./scripts/otf2ompprint2paje.pl $trace_file |
        pj_dump --user-defined |
        grep ^State > ${trace_file}.csv
done
#+end_src

Then, lets put that generated CSV in an acceptable shape:

#+begin_src R :session :results output :exports both
options(crayon.enabled = FALSE)
supressMessages(library(tidyverse))
supressMessages(library(functional))

libgomp_csv <- function(trace_file, out_file) {
  trace_file %>%
    read_csv(col_names = FALSE, col_types = cols(), trim_ws = TRUE) %>%
    rename(worker = X2,
           start = X4,
           end = X5,
           duration = X6,
           imbrication = X7,
           value = X8,
           job.id = X9) %>%
    write_csv(out_file)
}

data_dir <- "experiments/exp04/data"

data_dir %>%
  list.files(pattern = glob2rx("scorep*.trace.csv"),
             recursive = TRUE, full.names = TRUE) %>%
  mapply(., FUN = libomp_csv, .)
#+end_src

**** starpu -> csv

Here, we'll use =starvz=! Given that =starvz= presumes the default file name for
StarPU traces, we'll do some symbolic links in order to analyze those traces.

#+begin_src bash :exports both :results output
set -euo pipefail

process_trace() {
    mkdir -p $2
    pushd $2
    ln -s $1 prof_file_${USER}_0
    # starvz
    # mv files that starvz created
    popd
    rm -rf $2
}

DATA_DIR=$(readlink -f experiments/exp04/data/)
TRACES_STARPU=$(find $DATA_DIR -type f -wholename '*/results/starpu/*.trace')

cd $DATA_DIR

i=0
for file in $TRACES_STARPU; do
    process_trace $file tmp_$((i++)) &
done

wait
#+end_src


* Resources
:PROPERTIES:
:ATTACH_DIR: resources/
:END:

Home to anything I like or think is relevant to the task at hand.

** Archive

Home to old stuff

*** Deps script backup

#+begin_src bash :shebang "#!/bin/bash" :tangle experiments/exp01/deps.sh
#SBATCH --time=3:00:00
#SBATCH --chdir=.
#SBATCH --output=/home/users/hcpsilva/slurm_outputs/%x_%j.out
#SBATCH --error=/home/users/hcpsilva/slurm_outputs/%x_%j.err
#SBATCH --mail-type=END,FAIL
#SBATCH --mail-user=hcpsilva@inf.ufrgs.br

# more robust script
set -euo pipefail

INSTALL_DIR=$1/$SLURM_JOB_PARTITION
SPACK_DIR=${2:-$HOME/spack-erad}

pushd $HOME

if [ ! -d $SPACK_DIR ]; then
    echo "spack not yet installed!"
    git clone http://gitlab+deploy-token-127235:BZMob8RJoRPZAdLtsstX@gitlab.com/viniciusvgp/customSpack.git $SPACK_DIR
    pushd $SPACK_DIR
    ./install_spack.sh -symr
    popd
fi

. $SPACK_DIR/src/spack/share/spack/setup-env.sh

# find available compilers for this machine
spack compiler find

# get current node info
ARCH=$(spack arch)

# create the install dir if there isn't one
[ ! -d $INSTALL_DIR ] && mkdir -p $INSTALL_DIR

pushd $INSTALL_DIR

if [ ! -d openblas-0.3.7 ]; then
    echo "OpenBLAS not yet installed!"
    mkdir openblas-0.3.7
    spack install openblas@0.3.7 arch=$ARCH
    spack view -d true soft openblas-0.3.7 openblas@0.3.7 arch=$ARCH
fi

if [ ! -d hdf5-1.10.5 ]; then
    echo "HDF5 not yet installed!"
    mkdir hdf5-1.10.5
    spack install hdf5@1.10.5 arch=$ARCH
    spack view -d true soft hdf5-1.10.5 hdf5@1.10.5 arch=$ARCH
fi

if [ ! -d starpu-1.3.1 ]; then
    echo "StarPU not yet installed!"
    mkdir starpu-1.3.1
    spack install starpu@1.3.1~fxt~poti~examples~mpi+openmp arch=$ARCH
    spack view -d true soft starpu-1.3.1 starpu@1.3.1~fxt~poti~examples~mpi+openmp arch=$ARCH
fi

if [ ! -d netlib-lapack-3.8.0 ]; then
    echo "lapack not yet installed!"
    mkdir netlib-lapack-3.8.0
    spack install netlib-lapack@3.8.0 arch=$ARCH
    spack view -d true soft netlib-lapack-3.8.0 netlib-lapack@3.8.0 arch=$ARCH
fi

if [ ! -d libomp-6.0 ]; then
    echo "libomp not yet installed!"
    pip install --user lit
    mkdir libomp-6.0
    git clone https://github.com/llvm-mirror/openmp.git libomp-6.0/openmp
    pushd libomp-6.0/openmp
    git checkout release_60
    mkdir build
    pushd build
    LLVM_PATHS=$(find /usr/lib -name 'llvm-[0-9]*' | sed -e 's/$/\/bin/' | paste -s -d':' -)
    export PATH+=:$LLVM_PATHS
    cmake -DCMAKE_C_COMPILER=clang -DCMAKE_CXX_COMPILER=clang++ -DCMAKE_INSTALL_PREFIX=$INSTALL_DIR/libomp-6.0 -DLIBOMP_OMPT_SUPPORT=on -DLIBOMP_OMPT_OPTIONAL=on -DLIBOMP_STATS=on ..
    make -j
    make -j install
    popd
    popd
fi

if [ ! -d libkomp-master ]; then
    echo "libkomp not yet installed!"
    mkdir libkomp-master
    spack install --keep-stage libkomp@master+the+affinity+numa~tracing~papi+vardep arch=$ARCH
    spack view -d true soft libkomp-master libkomp@master+the+affinity+numa~tracing~papi+vardep arch=$ARCH
fi

if [ ! -d kstar-starpu-master ]; then
    echo "kstar not yet installed!"
    mkdir kstar-starpu-master
    spack install --keep-stage kstar@master+starpu^starpu@1.3.1~fxt~poti~examples~mpi+openmp arch=$ARCH
    spack view -d true soft kstar-starpu-master kstar@master+starpu^starpu@1.3.1~fxt~poti~examples~mpi+openmp arch=$ARCH
fi

popd
popd
#+end_src
