#+title: =qr-analysis= Lab-book
#+author: Henrique Silva
#+email: hcpsilva@inf.ufrgs.br
#+infojs_opt:
#+property: session *R*
#+property: cache yes
#+property: results graphics
#+property: exports both
#+property: tangle yes
#+seq_todo: TODO(t!) STARTED(s!) WAITING(w!) | DONE(d!) CANCELLED(c!) DEFERRED(f!)

Lab-book intended to document my efforts into extending Marcelo's
assignment/paper about QR Factorization analysis.

* Table of contents                                                   :TOC_3:
- [[#journal][Journal]]
  - [[#to-do][To-do]]
    - [[#active-items][Active items]]
    - [[#secondary-items][Secondary items]]
    - [[#archive][Archive]]
  - [[#daily][Daily]]
    - [[#2020-01-03][2020-01-03]]
    - [[#2020-01-12][2020-01-12]]
    - [[#2020-01-13][2020-01-13]]
    - [[#2020-01-14][2020-01-14]]
  - [[#meetings][Meetings]]
- [[#experiments][Experiments]]
  - [[#preamble][Preamble]]
    - [[#launching-the-experiments][Launching the experiments]]
    - [[#installing-dependencies][Installing dependencies]]
  - [[#1---overview---makespan-only][1 - Overview - makespan only]]
    - [[#design][Design]]
    - [[#dependencies][Dependencies]]
    - [[#makefile][Makefile]]
    - [[#script][Script]]
    - [[#analysis][Analysis]]
  - [[#2---tracing][2 - Tracing]]
    - [[#design-1][Design]]
    - [[#dependencies-1][Dependencies]]
    - [[#makefile-1][Makefile]]
    - [[#script-1][Script]]
    - [[#post-processing][Post-processing]]
    - [[#analysis-1][Analysis]]
- [[#resources][Resources]]
  - [[#archive-1][Archive]]
    - [[#deps-script-backup][Deps script backup]]

* Journal
:PROPERTIES:
:ATTACH_DIR: attachments/
:END:

Intended to hold any meaningful advance yet not meaningful enough to deserve its
own topic in my lab-book.

** To-do

*** Active items

**** TODO Run the first experiment!
- State "TODO"       from              [2020-01-03 sex 14:15]

As its kinda time-critical right now.

***** DONE Split the experiment design
CLOSED: [2020-01-10 sex 18:54]
- State "DONE"       from "TODO"       [2020-01-10 sex 18:54]
- State "TODO"       from              [2020-01-09 qui 17:12]

**** TODO Find out what's happening with the =kstar= version
- State "TODO"       from              [2020-01-12 dom 19:28]

It's the only version that isn't working.

**** TODO Remove OMPT support for EXP01
- State "TODO"       from              [2020-01-16 qui 17:05]

*** Secondary items

Empty! :)

*** Archive

**** DONE Talk to Marcelo about the script
CLOSED: [2019-10-16 qua 17:31]
- State "DONE"       from "TODO"       [2019-10-16 qua 17:31]
- State "TODO"       from              [2019-10-05 s√°b 20:14]

** Daily

Daily thoughts and developments.

*** 2020-01-03

First day of this repo!

*** 2020-01-12

I've observed the following behavior when trying to execute the =kstar starpu=
version:

#+begin_example
[starpu][read_a_places_list] Error: colon support unimplemented in numeric place list

/home/users/hcpsilva/Installs/spack/cei/starpu-1.3.1/lib/libstarpu-1.3.so.0(+0x9b3ef)[0x7f977b4753ef]
/home/users/hcpsilva/Installs/spack/cei/starpu-1.3.1/lib/libstarpu-1.3.so.0(_starpu_omp_environment_init+0x13)[0x7f977b475993]
/home/users/hcpsilva/Installs/spack/cei/starpu-1.3.1/lib/libstarpu-1.3.so.0(starpu_omp_init+0x143)[0x7f977b470403]
/home/users/hcpsilva/Installs/spack/cei/kstar-master/lib/libkstar_starpu.so.1(+0x33c9)[0x7f977b4ee3c9]
/lib64/ld-linux-x86-64.so.2(+0xf37a)[0x7f977bfd737a]
/lib64/ld-linux-x86-64.so.2(+0xf476)[0x7f977bfd7476]
/lib64/ld-linux-x86-64.so.2(+0x10ca)[0x7f977bfc90ca]
[starpu][abort][read_a_places_list()@util/openmp_runtime_support_environment.c:444]
Aborted
#+end_example

I don't get why this is happening, but as of now this is the only version which
fails to execute. Some notes on that:

- All libraries are properly linked;
- The backtrace does originate from =libkstar_starpu.so=;
- The =starpu= version is working without error.

I've also fixed a list of issues I've found while trying to execute the
experiment over the course of this weekend. I'm well aware that these events
certainly hinder the progress of this article, I must also note that I'm not
feeling as well as I'd hope to fully commit to this task.

I hope that this issue gets fixed by tomorrow, when I get in touch with
Vinicius.

--

Some curious behavior also happened when executing =libkomp=: no errors, but
some parameter combinations are taking longer than 5h to execute sometimes.

*** 2020-01-13

Interesting read: [[https://bastibe.de/2014-09-23-org-cite.html]]

*** 2020-01-14

I'm so tired but I found out what was happening with =kstar=:
This runtime is yet to implement the comma-separated core-list for the
environment variable =OMP_PLACES=. This variable currently can take the
following values:

- cores :: placement occurs to every /core/ available
- threads :: placement occurs to every /thread/ available
- sockets :: placement occurs to every /socket/ available, in a round-robin
             fashion
- ={list,of,cores}= :: supports multiple syntaxes, was the way we were doing
     things.

So, as we intend on evading SMT, I've set this variable to =cores=, and I've
observed that we still obtain the desired behavior and, as a bonus, =kstar= now
works.

Also, I've noted that the timeout issue still occurs when multiple nodes are
trying to install the experiment dependencies. I'll run everything again, just
to be sure.

--

While that issue has been resolved, we are still facing this other one:

#+begin_example
Error from /home/users/hcpsilva/spack-erad/src/spack/opt/spack/linux-debian10-skylake_avx512/gcc-8.3.0/kstar-master-6f46dnqwbgqazcyqud7ldawevt6akn35/bin/kstar-omp:
In file included from /scratch/hcpsilva/exp01_cei5_48/code/src/block_qr_openmp.c:1:
In file included from src/mutils/mutils.h:3:
/usr/include/stdio.h:33:10: fatal error: 'stddef.h' file not found
#include <stddef.h>
         ^
kstar-omp: frontend/InclusionRewriter.cpp:184: virtual void {anonymous}::InclusionRewriter::InclusionDirective(clang::SourceLocation, const clang::Token&, llvm::StringRef, bool, clang::CharSourceRange, const clang::FileEntry*, llvm::StringRef, llvm::StringRef, const clang::Module*): Assertion `LastInclusionLocation.isInvalid() && "Another inclusion directive was found before the previous one " "was processed"' failed.

make: *** [Makefile:90: bin/block_qr_kstar_starpu.o] Error 250
#+end_example

Particularly cryptic because it doesn't make any sense.

** Meetings

Like daily entries, but for meetings.

* Experiments

Hi! So, as I'll put into further details later, I use a script to run
experiments.

The so called "experiment IDs" are both sub-directories in the experiments
directory and tags in subsections of this section.

** Preamble

Important info if you are looking to reproduce my way of doing things.

Firstly though, I'd like you to read about how I run these experiments myself:

#+begin_src bash :exports both
cd <COMMON_DIRECTORY>/qr_analysis

./experiments/launch.sh --help
#+end_src

*** Launching the experiments

Hello! I suppose you've read the info in the previous section, so, here's the
deal: I use a script to run experiments.

Here's the maximum value of the elements in the matrices we'll deal with:

#+name: values_range
#+begin_src bash :results output :exports results
echo 100
#+end_src

#+RESULTS: values_range
: 100

It's big, I know, but it ensures that they run in the right nodes and it
installs all needed dependencies! Take a look at it:

#+begin_src bash :shebang "#!/bin/bash" :results none :tangle experiments/launch.sh
# more robust script
set -euo pipefail

function usage {
cat << EOF
  $0 [OPTIONS] <EXP_ID> [REPO_DIRECTORY]

  WHERE <EXP_ID> is the identificator of the experiment

  WHERE [OPTIONS] can be any of the following, in no particular order:
    -h | --help
      shows this message and exits
    -d | --dry
      prints what it would do instead of actually doing it
    -a | --available
      only use currently available nodes
    -u | --update
      updates the repo before running any commands
    -i | --install[=]path/to/the/installs
      use another dir instead of the default $HOME/Installs/spack
    --spack[=]path/to/spack
      use another spack dir instead of the default $HOME/spack-erad
    -p | --partitions[=]list,of,partitions,comma,separated
      define the desired partitions to be used (default: cei)
    -s | --split
      split the execution plan between all nodes that ought to be used
      WARNING: the splitting will occur between same-partition nodes only
               i.e.: if more than one partition is listed, we'll repeat
               the process in the other partitions
    -n | --nodes[=]list,of,nodes,comma,separated
      define the desired nodes to be used
      WARNING: this option disables usage of the partition list!
    -l | --local
      install all packages locally in each machine
      WARNING: probably won't work because of timeouts in spack
    -o | --overwrite
      overwrite the install of all packages listed as a dependency
    -O | --strong-overwrite
      force the reinstall of all packages listed as a dependency

  WHERE [REPO_DIRECTORY] is the *full* path to the repository
    It is presumed that you are in it, if you don't provide this argument
EOF
}

for i in "$@"; do
    case $i in
        -h|--help)
            echo "USAGE:"
            usage
            exit 0
            ;;
        --dry)
            DRY=echo
            shift
            ;;
        --update)
            git pull
            shift
            ;;
        --install=*)
            INSTALL_DIR=${i#*=}
            shift
            ;;
        --install)
            shift
            INSTALL_DIR=$1
            shift
            ;;
        --spack=*)
            SPACK_DIR=${i#*=}
            shift
            ;;
        --spack)
            shift
            SPACK_DIR=$1
            shift
            ;;
        --overwrite)
            OVERWRITE=true
            shift
            ;;
        --strong-overwrite)
            OVERWRITE=strong
            shift
            ;;
        --available)
            AVAILABLE="-t idle"
            shift
            ;;
        --split)
            SPLIT=true
            shift
            ;;
        --partitions=*)
            PARTITIONLIST=$(tr ',' ' ' <<<${i#*=})
            shift
            ;;
        --partitions)
            shift
            PARTITIONLIST=$(tr ',' ' ' <<<$1)
            shift
            ;;
        --nodes=*)
            NODELIST=$(tr ',' '\n' <<<${i#*=})
            PARTITIONLIST=$(sed -E 's/([0-9]+)//g' <<<$NODELIST | uniq | xargs)
            shift
            ;;
        --nodes)
            shift
            NODELIST=$(tr ',' '\n' <<<$1)
            PARTITIONLIST=$(sed -E 's/([0-9]+)//g' <<<$NODELIST | uniq | xargs)
            shift
            ;;
        --local)
            INSTALL_DIR=/scratch/$USER/.installs
            SPACK_DIR=/scratch/$USER/.spack
            LOCAL=true
            shift
            ;;
        --*)
            echo "ERROR: Unknown long option '$i'"
            echo
            echo "USAGE:"
            usage
            exit 1
            ;;
        -*)
            options=$(sed 's/./& /g' <<<${i#-})
            for letter in $options; do
                case $letter in
                    d)
                        DRY=echo
                        ;;
                    u)
                        git pull
                        ;;
                    i)
                        shift
                        INSTALL_DIR=$1
                        ;;
                    o)
                        OVERWRITE=true
                        ;;
                    O)
                        OVERWRITE=strong
                        ;;
                    a)
                        AVAILABLE="-t idle"
                        ;;
                    s)
                        SPLIT=true
                        ;;
                    p)
                        shift
                        PARTITIONLIST=$(tr ',' ' ' <<<$1)
                        ;;
                    n)
                        shift
                        NODELIST=$(tr ',' '\n' <<<$1)
                        PARTITIONLIST=$(sed -E 's/([0-9]+)//g' <<<$NODELIST | uniq | xargs)
                        ;;
                    l)
                        INSTALL_DIR=/scratch/$USER/.installs
                        SPACK_DIR=/scratch/$USER/.spack
                        LOCAL=true
                        ;;
                    ,*)
                        echo "ERROR: Unknown short option '-${letter}'"
                        echo
                        echo "USAGE:"
                        usage
                        exit 1
                        ;;
                esac
            done
            shift
            ;;
    esac
done

# directory with needed dependencies installed
INSTALL_DIR=${INSTALL_DIR:-$HOME/Installs/spack}

# the experiment id
EXPERIMENT_ID=$1

# the work (repo) dir
REPO_DIR=${2:-$PWD}

# default run partition
PARTITIONLIST=${PARTITIONLIST:-cei}

# local install boolean
LOCAL=${LOCAL:-false}

# the split plan boolean
SPLIT=${SPLIT:-false}

# overwrite the packages?
OVERWRITE=${OVERWRITE:-false}

# the path to the spack installation
SPACK_DIR=${SPACK_DIR:-$HOME/spack-erad}

if [[ $REPO_DIR != /* ]]; then
    echo "ERROR: Path to repository is not absolute, please use the absolute path..."
    exit 2
fi

if [[ $INSTALL_DIR != /* ]]; then
    echo "ERROR: Path to installation dir is not absolute, please use the absolute path..."
    exit 2
fi

if [[ $SPACK_DIR != /* ]]; then
    echo "ERROR: Path to spack isn't absolute, please use the absolute path..."
    exit 2
fi

EXP_DIR=$(find $REPO_DIR -type d -path "*/experiments/$EXPERIMENT_ID")
if [ ! -n "$EXP_DIR" ]; then
    echo "ERROR: There isn't any experiment with this ID..."
    exit 3
fi

pushd $REPO_DIR

for partition in $PARTITIONLIST; do
    # lets install all needed dependencies first
    if [ $LOCAL = false ]; then
        echo "-> Launching dependency installing job for partition $partition!"

        INSTALL_DIR+=/$partition # as we are not running locally
        ${DRY:-} sbatch \
            -p ${partition} \
            -N 1 \
            -J dependencies_${EXPERIMENT_ID}_${partition} \
            -W \
            $(dirname $EXP_DIR)/deps.sh $INSTALL_DIR $EXP_DIR $SPACK_DIR $OVERWRITE
        echo "... and done!"
    fi
    echo

    # change the gppd-info to sinfo when porting
    ALLNODES=$(gppd-info --long --Node -S NODELIST -p $partition -h ${AVAILABLE:-} | awk '{print $1}')
    if [ -z ${NODELIST+x} ]; then
        nodes=$(paste -s -d" " - <<<$ALLNODES)
    else
        nodes=$(grep "$NODELIST" <<<$ALLNODES | paste -s -d" " -)
    fi

    # splits the plan if we were told to
    if [ $SPLIT = true ]; then
        num_nodes=$(wc -w <<<$nodes)
        ${DRY:-} rm -f $EXP_DIR/runs.plan.${partition}.*
        ${DRY:-} split -n l/$num_nodes -d -a 1 $EXP_DIR/runs.plan $EXP_DIR/runs.plan.${partition}.
    fi

    # counter to access the correct plan
    plan_part=${num_nodes:+0}

    echo "-> Launching jobs for partition $partition!"

    for node in $nodes; do
        # if we are in local mode, install dependencies for this node
        if [ $LOCAL = true ]; then
            echo "Launching installation job locally for node ${node}..."
            ${DRY:-} sbatch \
                -p ${partition} \
                -w ${node} \
                -J dependencies_${EXPERIMENT_ID}_${node} \
                $(dirname $EXP_DIR)/deps.sh $INSTALL_DIR $EXP_DIR $SPACK_DIR $OVERWRITE
        fi

        # launch the slurm script for this node
        echo "Launching job for node ${node}..."
        ${DRY:-} sbatch \
            -p ${partition} \
            -w ${node} \
            -J qr_analysis_${EXPERIMENT_ID} \
            $EXP_DIR/exp.slurm $EXPERIMENT_ID $EXP_DIR $INSTALL_DIR ${plan_part:-}

        if [ ! -z ${plan_part:+z} ]; then
            plan_part=$((plan_part+1))
        fi
        echo
    done

    # if not local, revert the path so we can repeat to the next partition
    [ $LOCAL = false ] && INSTALL_DIR=$(dirname $INSTALL_DIR)

    echo
done

popd
#+end_src

*** Installing dependencies

Here shall lie the automatic dependencies installer...

#+begin_src bash :shebang "#!/bin/bash" :results none :tangle experiments/deps.sh
#SBATCH --time=3:00:00
#SBATCH --chdir=.
#SBATCH --output=/home/users/hcpsilva/slurm_outputs/%x_%j.out
#SBATCH --error=/home/users/hcpsilva/slurm_outputs/%x_%j.err
#SBATCH --mail-type=END,FAIL
#SBATCH --mail-user=hcpsilva@inf.ufrgs.br

# more robust script
set -euo pipefail

# to install spack dependencies
function spack_install_spec {
    SPEC=$1
    ARCH=$2
    OVER=$3

    name_version=${SPEC%%[~|+|^]*}
    dir_name=$PWD/$(tr '@' '-' <<<$name_version)

    # if we fall here, we have already installed the package
    [ -d $dir_name/lib ] && [ $OVER = false ] && return 0

    flags="--keep-stage -y"

    echo "${name_version} not yet installed!"
    if [ $OVER != false ]; then
        rm -rf $dir_name
        flags+=" --overwrite"
    fi

    [ $OVER = strong ] && spack uninstall -fy $SPEC arch=$ARCH

    mkdir -p $dir_name
    spack install $flags $SPEC arch=$ARCH
    spack view -d true soft -i $dir_name $SPEC arch=$ARCH
    spack find -l

    [ ! -f installs.log ] && echo "SPECS HERE INSTALLED" > installs.log
    echo >> installs.log
    echo -e "PACKAGE:\t${name_version}" >> installs.log
    echo -e "SPEC:\t${SPEC}" >> installs.log
}

function source_install_spec {
    SPEC=$1
    EXP_DIR=$2
    OVER=$3

    name_version=$(tr '@' '-' <<<${SPEC%%[~|+|^]*})
    prefix=$PWD/$name_version
    repo=$prefix/repo

    # if we fall here, we have already installed the package
    [ -d $prefix/lib ] && [ $OVER = false ] && exit 0

    [ $OVER != false ] && rm -rf $prefix

    echo "${name_version} not yet installed!"

    # install by the provided shell install script
    mkdir -p $prefix
    ${EXP_DIR}/${name_version}.sh $prefix/ $repo

    [ ! -f installs.log ] && echo "SPECS HERE INSTALLED" > installs.log
    echo >> installs.log
    echo -e "PACKAGE:\t${name_version}" >> installs.log
    echo -e "SPEC:\t${SPEC}" >> installs.log
}

INSTALL_DIR=$1
EXP_DIR=$2
SPACK_DIR=$3
OVERWRITE=$4

pushd $HOME

if [ ! -d $SPACK_DIR ]; then
    echo "spack not yet installed!"
    git clone http://gitlab+deploy-token-127235:BZMob8RJoRPZAdLtsstX@gitlab.com/viniciusvgp/customSpack.git $SPACK_DIR
    pushd $SPACK_DIR
    ./install_spack.sh -symr
    popd
fi

. $SPACK_DIR/src/spack/share/spack/setup-env.sh

# find available compilers for this machine
spack compiler find

# get current node info
arch=$(spack arch)

# create the install dir if there isn't one
[ ! -d $INSTALL_DIR ] && mkdir -p $INSTALL_DIR

pushd $INSTALL_DIR

echo "--> INSTALLING DEPENDENCIES"

while read -r method spec; do
    echo $method $spec

    case $method in
        spack)
            spack_install_spec $spec $arch $OVERWRITE
            ;;
        manual)
            source_install_spec $spec $EXP_DIR $OVERWRITE
            ;;
        ,*)
            echo
            echo "ERROR: method not supported..."
            exit 128
            ;;
    esac
done < $EXP_DIR/exp.deps

echo
echo "--> DONE"

popd
popd
#+end_src

** WAITING 1 - Overview - makespan only                              :EXP01:
- State "WAITING"    from "STARTED"    [2020-01-09 qui 15:22]
- State "STARTED"    from              [2020-01-09 qui 15:22]

Only a makespan analysis of all different runtime options, no tracing involved

*UPDATE*: 2020-01-15

Utilized following partitions and nodes

| Partition | Nodes |
|-----------+-------|
| cei       |   2:8 |
| hype      |   3:5 |
| draco     |   1:5 |

*** Design

The random seed will be:

#+begin_src R :session :results value :exports results
floor(runif(1,1,99999))
#+end_src

#+RESULTS:
: 86229

Finally, the design itself:

#+begin_src R :session :results output :var expKey="exp01"
suppressMessages(library(tidyverse))
suppressMessages(library(DoE.base))

matrix = c(1024, 2048, 4096, 8192, 16384, 32768)
nb = c(32, 64, 128, 256, 512)
method = c("starpu", "libomp", "libgomp", "libkomp_gcc", "libkomp_clang", "kstar_starpu")

complete <- fac.design(
  nfactors=3,
  replications=5,
  repeat.only=FALSE,
  blocks=1,
  randomize=TRUE,
  seed=86229,
  factor.names=list(
    matrix_size=matrix,
    block_size=nb,
    runtime=method)) %>%
  as_tibble %>%
  filter(matrix_size == 8192) %>%
  transmute(id=as.numeric(Blocks), runtime, matrix_size, block_size) %>%
  write_delim(paste0("experiments/", expKey, "/runs.plan"), delim=" ", col_names=FALSE)

# the space delimited file is to help with the posterior parsing in the shell
# script
#+end_src

#+RESULTS:
:
: creating full factorial with 180 runs ...

*** Dependencies

In this experiment we'll need the following =spack= packages:

- starpu-1.3
- llvm
- netlib-lapack
- libkomp
- kstar-starpu

#+begin_src text :exports both :tangle experiments/exp01/exp.deps
spack hdf5@1.10.5~mpi
spack starpu@1.3.1~fxt~poti~examples~mpi+openmp
spack netlib-lapack@3.8.0
manual libomp@6.0
spack libkomp@master+the+affinity+numa~tracing~papi+vardep
spack kstar@master+starpu^starpu@1.3.1~fxt~poti~examples~mpi+openmp
#+end_src

**** libomp-6.0

From the [[https://github.com/llvm-mirror/openmp][LLVM stdlib]]:

#+begin_src bash :shebang "#!/bin/bash" :exports both :results none :tangle experiments/exp01/libomp-6.0.sh
INSTALL_DIR=$1
REPO_DIR=$2

pip install --user lit
git clone https://github.com/llvm-mirror/openmp.git $REPO_DIR
pushd $REPO_DIR
git checkout release_60
mkdir build
pushd build
LLVM_PATHS=$(find /usr/lib -name 'llvm-[0-9]*' | sed -e 's/$/\/bin/' | paste -s -d':' -)
export PATH+=:$LLVM_PATHS
cmake \
    -DCMAKE_C_COMPILER=clang \
    -DCMAKE_CXX_COMPILER=clang++ \
    -DCMAKE_INSTALL_PREFIX=$INSTALL_DIR \
    ..
make -j
make -j install
popd
popd
#+end_src

*** Makefile

The =makefile= used in this experiment!

#+begin_src makefile :tangle experiments/exp01/code/Makefile
OBJ_DIR := bin
OUT_DIR := build
SRC_DIR := src
LIB_DIR := lib
INC_DIR := include

DEBUG :=

LIB_EXTRA :=
INC_EXTRA :=

#	- Compilation flags:
#	Compiler and language version
CC := gcc
KSTAR := kstar --runtime starpu
DEBUGF := $(if $(DEBUG),-g -fsanitize=address)
CFLAGS :=\
	-Wall \
	-Wextra \
	-Wpedantic \
	-Wshadow \
	-Wunreachable-code
OMP := -fopenmp
OPT := $(if $(DEBUG),-O0,-O2 -march=native)
LIB := -L$(LIB_DIR) $(LIB_EXTRA)\
	$(shell pkg-config lapack lapacke blas --libs)\
	-lm
INC := -I$(INC_DIR) -I$(SRC_DIR) $(INC_EXTRA)\
	$(shell pkg-config lapack lapacke blas --cflags)

#	Should be defined in the command line
LIBOMP_PATH :=
LIBOMP := -L$(LIBOMP_PATH)/lib -Wl,--rpath,$(LIBOMP_PATH)/lib -I$(LIBOMP_PATH)/include

################################################################################
#	Files:

#	- Path to all final binaries:
TARGET := $(OUT_DIR)/block_qr_libgomp $(OUT_DIR)/block_qr_starpu $(OUT_DIR)/block_qr_libomp $(OUT_DIR)/matrix_generator $(OUT_DIR)/block_qr_kstar_starpu

################################################################################
#	Targets:

.DEFAULT_GOAL = all

all: $(TARGET)

#
# mutils
#
$(OBJ_DIR)/mutils.o: $(SRC_DIR)/mutils/mutils.c
	$(CC) -c -o $@ $^ $(INC) $(CFLAGS)

$(OBJ_DIR)/mutils_kstar.o: $(SRC_DIR)/mutils/mutils.c
	$(KSTAR) -c -o $@ $^ $(INC) $(CFLAGS)

#
# OPENMP task based parallel blocked QR factorization
#
$(OBJ_DIR)/block_qr_libgomp.o: $(SRC_DIR)/block_qr_openmp.c
	$(CC) -c -o $@ $^ $(INC) $(OMP) $(CFLAGS)

$(OUT_DIR)/block_qr_libgomp: $(OBJ_DIR)/block_qr_libgomp.o $(OBJ_DIR)/mutils.o
	$(CC) -o $@ $^ $(OMP) $(LIB)

#
# STARPU task based parallel blocked QR factorization
#
$(OBJ_DIR)/block_qr_starpu.o: $(SRC_DIR)/block_qr_starpu.c
	$(CC) -c -o $@ $^ $(INC) $(shell pkg-config starpu-1.3 --cflags) $(CFLAGS)

$(OUT_DIR)/block_qr_starpu: $(OBJ_DIR)/block_qr_starpu.o $(OBJ_DIR)/mutils.o
	$(CC) -o $@ $^ $(INC) $(shell pkg-config starpu-1.3 hwloc --libs --cflags) $(LIB)


#
# OpenMP with llvm runtime (libomp)
#
$(OBJ_DIR)/block_qr_libomp.o: $(SRC_DIR)/block_qr_openmp.c
	$(CC) -c -o $@ $^ $(INC) $(OMP) $(LIBOMP) $(CFLAGS)

$(OUT_DIR)/block_qr_libomp: $(OBJ_DIR)/block_qr_libomp.o $(OBJ_DIR)/mutils.o
	$(CC) -o $@ $^ $(LIB) $(OMP) $(LIBOMP)

#
# Kstar with starpu runtime
#
$(OBJ_DIR)/block_qr_kstar_starpu.o: $(SRC_DIR)/block_qr_openmp.c
	$(KSTAR) -c -o $@ $^ $(INC) $(OMP) $(CFLAGS)

$(OUT_DIR)/block_qr_kstar_starpu: $(OBJ_DIR)/block_qr_kstar_starpu.o $(OBJ_DIR)/mutils_kstar.o
	$(KSTAR) -o $@ $^ $(LIB) $(OMP)

#
# Matrix Generator
#
$(OUT_DIR)/matrix_generator: $(SRC_DIR)/matrix_generator.c
	$(CC) -o $@ $^ $(CFLAGS)

# misc

print-%:
	@echo $* = $($*)

clean:
	rm -f $(OBJ_DIR)/*.o $(INC_DIR)/*~ $(TARGET) $(LIB_DIR)/*.so *~ *.o
#+end_src

*** Script

I'm using my script as a base because his script is, well, not pretty.

#+begin_src bash :shebang "#!/bin/bash" :tangle experiments/exp01/exp.slurm
#SBATCH --time=24:00:00
#SBATCH --chdir=.
#SBATCH --output=/home/users/hcpsilva/slurm_outputs/%x_%j.out
#SBATCH --error=/home/users/hcpsilva/slurm_outputs/%x_%j.err
#SBATCH --mail-type=END,FAIL
#SBATCH --mail-user=hcpsilva@inf.ufrgs.br

# more robust script
# set -euo pipefail

# parameters:
# the experiment ID, defined in the lab-book
EXP_ID=$1
# the experiment directory
EXP_DIR=$2
# the path to the directory where we'll find the needed packages
INSTALL=$3
# are we splitting plans?
PLAN_SUFFIX=${4:+.${SLURM_JOB_PARTITION}.}${4:-}

# node name
HOST=$(hostname)

# maximum element value (defined in experiment design)
MAXVAL=100

# experiment name (which is the ID and the machine and its core count)
EXP_NAME=${EXP_ID}_${HOST}_${SLURM_CPUS_ON_NODE}

# seed generated in project design
RAND_SEED=86229

# go to the scratch dir to execute our operations
cd $SCRATCH

# clean up my scratch dir
rm -rf *

STARPU_PATH=$(readlink -f $INSTALL/starpu-1.3.1)
LIBOMP_PATH=$(readlink -f $INSTALL/libomp-6.0)
LAPACK_PATH=$(readlink -f $INSTALL/netlib-lapack-3.8.0)
HDF5_PATH=$(readlink -f $INSTALL/hdf5-1.10.5)
LIBKOMP_PATH=$(readlink -f $INSTALL/libkomp-master)
KSTAR_PATH=$(readlink -f $INSTALL/kstar-master)

PATH+=:$STARPU_PATH/bin
PATH+=:$KSTAR_PATH/bin
export PATH=$PATH

PKG_CONFIG_PATH+=:$STARPU_PATH/lib/pkgconfig
PKG_CONFIG_PATH+=:$LAPACK_PATH/lib/pkgconfig
export PKG_CONFIG_PATH=$PKG_CONFIG_PATH

# prepare env variables
threads_per_core=$(lscpu | grep "per core" | awk '{print $4}')
real_core_count=$((${SLURM_CPUS_ON_NODE} / ${threads_per_core:-1}))
export STARPU_NCPU=$real_core_count
export OMP_NUM_THREADS=$real_core_count
export STARPU_FXT_TRACE=0
export KAAPI_RECORD_TRACE=0
export OMP_PLACES=cores
export OMP_PROC_BIND=true
export KMP_STACKSIZE=$((1024*1024*34))

echo "Environment variables set up!"

# prepare our directory
mkdir $EXP_NAME
pushd $EXP_NAME

# copy the code folder
cp -r $EXP_DIR/code code
mkdir results

pushd code
make clean
make all LIBOMP_PATH="$LIBOMP_PATH"
ln -s $PWD/build/block_qr_libomp $PWD/build/block_qr_libkomp_clang
ln -s $PWD/build/block_qr_libgomp $PWD/build/block_qr_libkomp_gcc
popd

# init the results csv
results_csv=results/${HOST}_data.csv
echo "node,rep_id,matrix_size,block_size,runtime,compute_time,total_time" > $results_csv

# execute the experiment
while read -r id runtime matrix num_blocks; do
    echo "-> Parameters set to: $runtime $matrix $num_blocks"

    # output log file
    log_file=results/${runtime}_${matrix}_${num_blocks}_${id}.log

    # execute given runtime and log results

    LD_LIBRARY_PATH=$LAPACK_PATH/lib

    if [[ $runtime = starpu ]] || [[ $runtime = kstar_starpu ]]; then
        LD_LIBRARY_PATH+=:$HDF5_PATH/lib
        LD_LIBRARY_PATH+=:$STARPU_PATH/lib
    elif [[ $runtime = kstar_starpu ]]; then
        LD_LIBRARY_PATH+=:$KSTAR_PATH/lib
    elif [[ $runtime = openmp ]]; then
        LD_LIBRARY_PATH+=:$LIBOMP_PATH/lib
    elif [[ $runtime = libkomp_gcc ]] || [[ $runtime = libkomp_clang ]]; then
        LD_LIBRARY_PATH+=:$LIBKOMP_PATH/lib
    fi

    export LD_LIBRARY_PATH=$LD_LIBRARY_PATH

    timeout 1h ./code/build/block_qr_$runtime \
            $matrix \
            $num_blocks \
            $RAND_SEED \
            $MAXVAL > $log_file 2>&1

    # get compute and total times from output
    ctime=$(grep -w compute_time $log_file | awk '{print $2}')
    ttime=$(grep -w total_time $log_file | awk '{print $2}')

    # add the execution data to the csv
    echo ${HOST},${id},${matrix},${num_blocks},${runtime},${ctime},${ttime} >> $results_csv

    echo
done < $EXP_DIR/runs.plan${PLAN_SUFFIX:-}

# gather node info
./code/scripts/node_info.sh > env.node

# create the data dir if it isn't already there
[ ! -d $EXP_DIR/data ] && mkdir $EXP_DIR/data

# zip everything and commit to EXP_DIR
tar czf $EXP_DIR/data/${EXP_NAME}_data.tar.gz *

popd
rm -rf $SCRATCH/*
#+end_src

*** Analysis

So, for the analysis of this data, I intend to create a function to amass all
collected data.

#+begin_src bash :exports both :results output :dir experiments/exp01/data :var INPUT="cei draco hype"
partitions=($INPUT)

for part in ${partitions[@]}; do
    echo $part

    mkdir $part
    for file in *${part}*.tar.gz; do
        echo $file
        tar xzf $file -C $part
    done
done
#+end_src

#+RESULTS:
#+begin_example
cei
exp01_cei2_48_data.tar.gz
exp01_cei3_48_data.tar.gz
exp01_cei4_48_data.tar.gz
exp01_cei5_48_data.tar.gz
exp01_cei6_48_data.tar.gz
exp01_cei7_48_data.tar.gz
exp01_cei8_48_data.tar.gz
draco
exp01_draco1_32_data.tar.gz
exp01_draco2_32_data.tar.gz
exp01_draco3_32_data.tar.gz
exp01_draco4_32_data.tar.gz
exp01_draco5_32_data.tar.gz
hype
exp01_hype3_40_data.tar.gz
exp01_hype4_40_data.tar.gz
exp01_hype5_40_data.tar.gz
#+end_example

And then lets get all that csv data!

#+begin_src R :session :results output :exports both
options(crayon.enabled = FALSE)
library(functional)
library(tidyverse)

collect_exp <- function(filename, exp_name) {
  read_csv(filename) %>%
    mutate(expid = exp_name)
}

data_dir <- "experiments/exp01/data/"
partitions <- c("cei", "hype", "draco")

partitions %>%
  lapply(Curry(paste0, data_dir)) %>%
  paste0("/results") %>%
  list.files(pattern = glob2rx("*.csv"), full.names = TRUE) %>%
  lapply(Curry(collect_exp, exp_name = "exp01")) -> dfs
#+end_src

#+RESULTS:
#+begin_example

Parsed with column specification:
cols(
  node = col_character(),
  rep_id = col_double(),
  matrix_size = col_double(),
  block_size = col_double(),
  runtime = col_character(),
  compute_time = col_double(),
  total_time = col_double()
)
Parsed with column specification:
cols(
  node = col_character(),
  rep_id = col_double(),
  matrix_size = col_double(),
  block_size = col_double(),
  runtime = col_character(),
  compute_time = col_double(),
  total_time = col_double()
)
Parsed with column specification:
cols(
  node = col_character(),
  rep_id = col_double(),
  matrix_size = col_double(),
  block_size = col_double(),
  runtime = col_character(),
  compute_time = col_double(),
  total_time = col_double()
)
Parsed with column specification:
cols(
  node = col_character(),
  rep_id = col_double(),
  matrix_size = col_double(),
  block_size = col_double(),
  runtime = col_character(),
  compute_time = col_double(),
  total_time = col_double()
)
Parsed with column specification:
cols(
  node = col_character(),
  rep_id = col_double(),
  matrix_size = col_double(),
  block_size = col_double(),
  runtime = col_character(),
  compute_time = col_double(),
  total_time = col_double()
)
Parsed with column specification:
cols(
  node = col_character(),
  rep_id = col_double(),
  matrix_size = col_double(),
  block_size = col_double(),
  runtime = col_character(),
  compute_time = col_double(),
  total_time = col_double()
)
Parsed with column specification:
cols(
  node = col_character(),
  rep_id = col_double(),
  matrix_size = col_double(),
  block_size = col_double(),
  runtime = col_character(),
  compute_time = col_double(),
  total_time = col_double()
)
Parsed with column specification:
cols(
  node = col_character(),
  rep_id = col_double(),
  matrix_size = col_double(),
  block_size = col_double(),
  runtime = col_character(),
  compute_time = col_double(),
  total_time = col_double()
)
Parsed with column specification:
cols(
  node = col_character(),
  rep_id = col_double(),
  matrix_size = col_double(),
  block_size = col_double(),
  runtime = col_character(),
  compute_time = col_double(),
  total_time = col_double()
)
Parsed with column specification:
cols(
  node = col_character(),
  rep_id = col_double(),
  matrix_size = col_double(),
  block_size = col_double(),
  runtime = col_character(),
  compute_time = col_double(),
  total_time = col_double()
)
Parsed with column specification:
cols(
  node = col_character(),
  rep_id = col_double(),
  matrix_size = col_double(),
  block_size = col_double(),
  runtime = col_character(),
  compute_time = col_double(),
  total_time = col_double()
)
Parsed with column specification:
cols(
  node = col_character(),
  rep_id = col_double(),
  matrix_size = col_double(),
  block_size = col_double(),
  runtime = col_character(),
  compute_time = col_double(),
  total_time = col_double()
)
Parsed with column specification:
cols(
  node = col_character(),
  rep_id = col_double(),
  matrix_size = col_double(),
  block_size = col_double(),
  runtime = col_character(),
  compute_time = col_double(),
  total_time = col_double()
)
Parsed with column specification:
cols(
  node = col_character(),
  rep_id = col_double(),
  matrix_size = col_double(),
  block_size = col_double(),
  runtime = col_character(),
  compute_time = col_double(),
  total_time = col_double()
)
Parsed with column specification:
cols(
  node = col_character(),
  rep_id = col_double(),
  matrix_size = col_double(),
  block_size = col_double(),
  runtime = col_character(),
  compute_time = col_double(),
  total_time = col_double()
)
#+end_example

With all that data collected, lets put all of it in the same dataframe!

#+begin_src R :session :results output :exports both
dfs %>%
  reduce(bind_rows) %>%
  mutate(platform = str_remove(node, "[0-9]")) -> exp01_df

exp01_df
#+end_src

#+RESULTS:
#+begin_example

# A tibble: 450 x 9
   node  rep_id matrix_size block_size runtime compute_time total_time expid
   <
 <
      <
     <
<
         <
     <
<chr>
 1 cei2       1        8192        512 libomp         107.       108.  exp01
 2 cei2       1        8192         32 libkom‚Ä¶         NA         NA   exp01
 3 cei2       1        8192        256 libkom‚Ä¶         92.7       94.1 exp01
 4 cei2       1        8192        512 starpu         107.       109.  exp01
 5 cei2       1        8192        512 libkom‚Ä¶        107.       108.  exp01
 6 cei2       1        8192         64 libkom‚Ä¶        306.       307.  exp01
 7 cei2       1        8192         64 starpu         229.       231.  exp01
 8 cei2       1        8192        256 libgomp         86.7       88.2 exp01
 9 cei2       1        8192         32 libomp         761.       763.  exp01
10 cei2       1        8192         64 libgomp        240.       241.  exp01
# ‚Ä¶ with 440 more rows, and 1 more variable: platform <chr>
#+end_example

Now for some visualizations, I'll replicate the first figure from Marcelo's
paper.

#+begin_src R :session :results output :exports both
calculate_tasks <- function(x) {
  if(x == 1) return (1) else return (x*x + calculate_tasks(x-1))
}

exp01_df %>%
  group_by(platform, matrix_size, block_size, runtime) %>%
  summarize(avg_time = mean(compute_time),
            repetitions = n(),
            error = 3 * sd(compute_time) / sqrt(repetitions)) %>%
    rowwise() %>%
    mutate(num_tasks = calculate_tasks(matrix_size / block_size)) %>%
    ungroup() -> fig_1

fig_1
#+end_src

#+RESULTS:
#+begin_example

# A tibble: 90 x 8
   platform matrix_size block_size runtime  avg_time repetitions error num_tasks
   <chr>          <dbl>      <dbl> <chr>       <dbl>       <int> <dbl>     <dbl>
 1 cei             8192         32 kstar_s‚Ä¶     82.9           5  1.38   5625216
 2 cei             8192         32 libgomp     763.            5 40.3    5625216
 3 cei             8192         32 libkomp‚Ä¶     NA             5 NA      5625216
 4 cei             8192         32 libkomp‚Ä¶     NA             5 NA      5625216
 5 cei             8192         32 libomp      706.            5 52.9    5625216
 6 cei             8192         32 starpu      751.            5 63.9    5625216
 7 cei             8192         64 kstar_s‚Ä¶     10.5           5  1.30    707264
 8 cei             8192         64 libgomp     241.            5  7.58    707264
 9 cei             8192         64 libkomp‚Ä¶    308.            5  8.02    707264
10 cei             8192         64 libkomp‚Ä¶    306.            5  6.39    707264
# ‚Ä¶ with 80 more rows
#+end_example

now for the plot...

#+begin_src R :session :results output :file (org-babel-temp-file "figure" ".png") :exports both :width 500 :height 300
fig_1 %>%
  ggplot(aes(x = as.factor(matrix_size),
             y = avg_time,
             color = as.factor(runtime))) +
  geom_point(size = 2,
             position=position_dodge(width = 1)) +
  geom_errorbar(aes(ymin = avg_time - error,
                    ymax = avg_time + error),
                width = 0.6,
                position=position_dodge(width = 1)) +
  labs(x = "Tamanho da matriz",
       y = "Tempo m√©dio de execu√ß√£o [seg]",
       color = "Runtime utilizado") +
  ylim(0, NA) +
  scale_y_continuous(expand = expand_scale(mult = c(0, 0.05)),
                     breaks = scales::pretty_breaks(n = 8)) +
  facet_grid(platform ~ block_size) +
  theme_bw() +
  theme(text = element_text(family = "Palatino", size = 16),
        legend.position = "top",
        axis.text.x = element_text(angle = 45, hjust = 1))
#+end_src

#+RESULTS:
:
: Scale for 'y' is already present. Adding another scale for 'y', which will
: replace the existing scale.
: Warning messages:
: 1: Removed 13 rows containing missing values (geom_point).
: 2: Removed 13 rows containing missing values (geom_errorbar).

there is inter-node variability...

#+begin_src R :session :results output graphics :file img/makespan-all.png :exports both :width 800 :height 600
suppressMessages(library(gtools))

fig_1 %>%
  mutate(case = factor(paste(platform, block_size, sep = " : "))) %>%
  mutate(case = factor(case, levels = mixedsort(levels(case)))) %>%
  ggplot(aes(x = as.factor(matrix_size),
             y = avg_time,
             color = as.factor(runtime),
             shape = as.factor(runtime))) +
  geom_point(size = 2,
             position=position_dodge(width = 1)) +
  geom_errorbar(aes(ymin = avg_time - error,
                    ymax = avg_time + error),
                width = 0.5,
                position=position_dodge(width = 1)) +
  labs(x = "Tamanho da matriz",
       y = "Tempo m√©dio de execu√ß√£o [seg]",
       color = "Runtime utilizado",
       shape = "Runtime utilizado") +
  facet_wrap(~ case, scales="free", nrow = 3) +
  scale_y_continuous(expand = expand_scale(mult = c(0, 0.05)),
                     breaks = scales::pretty_breaks(n = 8),
                     limits = c(0, NA)) +
  theme_bw() +
  theme(text = element_text(family = "Palatino", size = 16),
        strip.text = element_text(size = 12),
        # strip.background = element_rect(fill = "white"),
        legend.position = "top")
#+end_src

#+RESULTS:

** STARTED 2 - Tracing
- State "STARTED"    from              [2020-01-09 qui 15:22]

Following the idea behind the third experiment, this experiment is a reduced
version of experiment 02! I'll use the same matrix size as the third experiment,
=8192=.

*** Design

Again we are going with an exhaustive design until second notice.

The random seed will be:

#+begin_src R :session :results value :exports results
floor(runif(1,1,99999))
#+end_src

#+RESULTS:
: 15195

And the design:

#+begin_src R :session :results none
suppressMessages(library(tidyverse))
suppressMessages(library(DoE.base))

matrix = c(1024, 2048, 4096, 8192, 16384, 32768)
nb = c(32, 64, 128, 256, 512)
method = c("starpu", "ompt", "scorep", "libkomp_gcc", "libkomp_clang", "kstar_starpu")

fac.design(
  nfactors=3,
  replications=1,
  repeat.only=FALSE,
  blocks=1,
  randomize=TRUE,
  seed=15195,
  factor.names=list(
    matrix_size=matrix,
    block_size=nb,
    runtime=method)) %>%
  as_tibble %>%
  filter(matrix_size == 8192) %>%
  transmute(runtime, matrix_size, block_size) %>%
  write_delim("experiments/exp02/runs.plan", delim=" ", col_names=FALSE)

# the space delimited file is to help with the posterior parsing in the shell
# script
#+end_src

*** Dependencies

In this experiment we'll need the following =spack= packages:

- starpu-1.3
- llvm
- netlib-lapack
- scorep

#+begin_src text :tangle experiments/exp02/exp.deps
spack hdf5@1.10.5~mpi
spack starpu@1.3.1+fxt+poti~examples~mpi+openmp
spack netlib-lapack@3.8.0
manual libomp@6.0
manual libomp-ompt@6.0
spack libkomp@master+the+affinity+numa+tracing+papi+vardep
spack kstar@master+starpu^starpu@1.3.1+fxt+poti~examples~mpi+openmp
spack scorep@6.0
#+end_src

**** libomp-ompt-6.0

From the [[https://github.com/llvm-mirror/openmp][LLVM stdlib]]:

#+begin_src bash :shebang "#!/bin/bash" :exports both :results none :tangle experiments/exp02/libomp-ompt-6.0.sh
INSTALL_DIR=$1
REPO_DIR=$2

pip install --user lit
git clone https://github.com/llvm-mirror/openmp.git $REPO_DIR
pushd $REPO_DIR
git checkout release_60
mkdir build
pushd build
LLVM_PATHS=$(find /usr/lib -name 'llvm-[0-9]*' | sed -e 's/$/\/bin/' | paste -s -d':' -)
export PATH+=:$LLVM_PATHS
cmake \
    -DCMAKE_C_COMPILER=clang \
    -DCMAKE_CXX_COMPILER=clang++ \
    -DCMAKE_INSTALL_PREFIX=$INSTALL_DIR \
    -DLIBOMP_OMPT_SUPPORT=on \
    -DLIBOMP_OMPT_OPTIONAL=on \
    -DLIBOMP_STATS=on \
    ..
make -j
make -j install
popd
popd
#+end_src

**** libomp-6.0

From the [[https://github.com/llvm-mirror/openmp][LLVM stdlib]]:

#+begin_src bash :shebang "#!/bin/bash" :exports both :results none :tangle experiments/exp02/libomp-6.0.sh
INSTALL_DIR=$1
REPO_DIR=$2

pip install --user lit
git clone https://github.com/llvm-mirror/openmp.git $REPO_DIR
pushd $REPO_DIR
git checkout release_60
mkdir build
pushd build
LLVM_PATHS=$(find /usr/lib -name 'llvm-[0-9]*' | sed -e 's/$/\/bin/' | paste -s -d':' -)
export PATH+=:$LLVM_PATHS
cmake \
    -DCMAKE_C_COMPILER=clang \
    -DCMAKE_CXX_COMPILER=clang++ \
    -DCMAKE_INSTALL_PREFIX=$INSTALL_DIR \
    ..
make -j
make -j install
popd
popd
#+end_src

*** Makefile

The =makefile= used in this experiment!

#+begin_src makefile :tangle experiments/exp02/code/Makefile
OBJ_DIR := bin
OUT_DIR := build
SRC_DIR := src
LIB_DIR := lib
INC_DIR := include

DEBUG :=

#	- Compilation flags:
#	Compiler and language version
CC := gcc
CLANG := clang
KSTAR := kstar --runtime starpu
PRELINK := scorep
DEBUGF := $(if $(DEBUG),-g -fsanitize=address)
CFLAGS :=\
	-Wall \
	-Wextra \
	-Wpedantic \
	-Wshadow \
	-Wunreachable-code
OMP := -fopenmp
OPT := $(if $(DEBUG),-O0,-O2 -march=native)
LIB := -L$(LIB_DIR) \
	$(shell pkg-config lapack lapacke blas --libs)\
	-lm
INC := -I$(INC_DIR) -I$(SRC_DIR) \
	$(shell pkg-config lapack lapacke blas --cflags)

# LIBOMP := -L$(LIBOMP_LIB) -Wl,--rpath,$(LIBOMP_LIB) -I$(LIBOMP_INC)

#	Should be defined in the command line
LIBOMP_PATH :=
LIBOMP := -L$(LIBOMP_PATH)/lib -Wl,--rpath,$(LIBOMP_PATH)/lib -I$(LIBOMP_PATH)/include
LIBOMP_OMPT_PATH :=
LIBOMP_OMPT := -L$(LIBOMP_OMPT_PATH)/lib -Wl,--rpath,$(LIBOMP_OMPT_PATH)/lib -I$(LIBOMP_OMPT_PATH)/include

################################################################################
#	Files:

#	- List of targets
TARGET_EXE := $(OUT_DIR)/block_qr_scorep $(OUT_DIR)/block_qr_starpu $(OUT_DIR)/block_qr_ompt $(OUT_DIR)/matrix_generator $(OUT_DIR)/block_qr_kstar_starpu $(OUT_DIR)/block_qr_libkomp_clang $(OUT_DIR)/block_qr_libkomp_gcc

#	- Path to all final libraries:
TARGET_LIB := $(patsubst %, $(LIB_DIR)/lib%.so, $(shell basename $(shell find $(LIB_DIR)/* -maxdepth 0 -type d)))

################################################################################
#	Targets:

.DEFAULT_GOAL = all

all: $(TARGET_LIB) $(TARGET_EXE)

#
# mutils
#
$(OBJ_DIR)/mutils.o: $(SRC_DIR)/mutils/mutils.c
	$(CC) -c -o $@ $^ $(INC) $(CFLAGS)

$(OBJ_DIR)/mutils_kstar.o: $(SRC_DIR)/mutils/mutils.c
	$(KSTAR) -c -o $@ $^ $(INC) $(CFLAGS)

#
# SCOREP - OPENMP task based parallel blocked QR factorization
#
$(OUT_DIR)/block_qr_scorep: $(SRC_DIR)/block_qr_openmp.c $(OBJ_DIR)/mutils.o
	$(PRELINK) $(CC) -o $@ $^ $(INC) $(OMP) $(LIB) $(CFLAGS)

#
# STARPU task based parallel blocked QR factorization
#
$(OUT_DIR)/block_qr_starpu: $(SRC_DIR)/block_qr_starpu.c $(OBJ_DIR)/mutils.o
	$(CC) -o $@ $^ $(INC) $(shell pkg-config starpu-1.3 hwloc fxt poti --libs --cflags) $(LIB) $(CFLAGS)

#
# Lib for getting OMPT traces
#
$(LIB_DIR)/libinit.so: $(LIB_DIR)/init/initialization.c $(LIB_DIR)/init/initialization.h
	$(CC) $^ -o $@ -shared -fPIC $(CFLAGS) $(LIBOMP_OMPT) $(OMP)

$(OUT_DIR)/block_qr_ompt: $(SRC_DIR)/block_qr_ompt.c $(OBJ_DIR)/mutils.o
	$(CC) -o $@ $^ $(INC) $(LIB) $(CFLAGS) $(OMP) $(LIBOMP_OMPT) -DDYN_TOOL -linit

#
# Kstar with starpu runtime
#
$(OUT_DIR)/block_qr_kstar_starpu: $(SRC_DIR)/block_qr_openmp.c $(OBJ_DIR)/mutils_kstar.o
	$(KSTAR) -o $@ $^ $(shell pkg-config starpu-1.3 hwloc fxt poti --libs) $(INC) $(LIB) $(OMP) $(CFLAGS)

#
# Libkomp
#
$(OUT_DIR)/block_qr_libkomp_clang: $(SRC_DIR)/block_qr_openmp.c $(OBJ_DIR)/mutils.o
	$(CLANG) -o $@ $^ $(INC) $(LIB) $(CFLAGS) $(OMP) $(LIBOMP) -g

$(OUT_DIR)/block_qr_libkomp_gcc: $(SRC_DIR)/block_qr_openmp.c $(OBJ_DIR)/mutils.o
	$(CC) -o $@ $^ $(INC) $(OMP) $(CFLAGS) $(LIB) -g

#
# Matrix Generator
#
$(OUT_DIR)/matrix_generator: $(SRC_DIR)/matrix_generator.c
	$(CC) -o $@ $^ $(CFLAGS)

print-%:
	@echo "$* == $($*)"

clean:
	rm -f $(OBJ_DIR)/*.o $(INC_DIR)/*~ $(OUT_DIR)/* $(TARGET_EXE) $(LIB_DIR)/*.so *~ *.o
#+end_src

*** Script

It's going to be based off the last script...

#+begin_src bash :shebang "#!/bin/bash" :tangle experiments/exp02/exp.slurm
#SBATCH --time=24:00:00
#SBATCH --chdir=.
#SBATCH --output=/home/users/hcpsilva/slurm_outputs/%x_%j.out
#SBATCH --error=/home/users/hcpsilva/slurm_outputs/%x_%j.err
#SBATCH --mail-type=END,FAIL
#SBATCH --mail-user=hcpsilva@inf.ufrgs.br

# more robust script
# set -euo pipefail

# parameters:
# the experiment ID, defined in the lab-book
EXP_ID=$1
# the experiment directory
EXP_DIR=$2
# the path to the directory where we'll find the needed packages
INSTALL=$3
# are we splitting plans?
PLAN_SUFFIX=${4:+.${SLURM_JOB_PARTITION}.}${4:-}

# node name
HOST=$(hostname)

# maximum element value (defined in experiment design)
MAXVAL=100

# experiment name (which is the ID and the machine and its core count)
EXP_NAME=${EXP_ID}_${HOST}_${SLURM_CPUS_ON_NODE}

# seed generated in project design
RAND_SEED=15195

# go to the scratch dir to execute our operations
cd $SCRATCH

# clean up my scratch dir
rm -rf *

STARPU_PATH=$(readlink -f $INSTALL/starpu-1.3.1)
LIBOMP_PATH=$(readlink -f $INSTALL/libomp-6.0)
LIBOMP_OMPT_PATH=$(readlink -f $INSTALL/libomp-ompt-6.0)
LAPACK_PATH=$(readlink -f $INSTALL/netlib-lapack-3.8.0)
SCOREP_PATH=$(readlink -f $INSTALL/scorep-6.0)
HDF5_PATH=$(readlink -f $INSTALL/hdf5-1.10.5)
LIBKOMP_PATH=$(readlink -f $INSTALL/libkomp-master)
KSTAR_PATH=$(readlink -f $INSTALL/kstar-master)

PATH+=:$STARPU_PATH/bin
PATH+=:$SCOREP_PATH/bin
PATH+=:$KSTAR_PATH/bin
PATH+=:$LIBKOMP_PATH/bin
export PATH=$PATH

PKG_CONFIG_PATH+=:$STARPU_PATH/lib/pkgconfig
PKG_CONFIG_PATH+=:$LAPACK_PATH/lib/pkgconfig
export PKG_CONFIG_PATH=$PKG_CONFIG_PATH

# prepare env variables
threads_per_core=$(lscpu | grep "per core" | awk '{print $4}')
real_core_count=$((${SLURM_CPUS_ON_NODE} / ${threads_per_core:-1}))

export STARPU_NCPU=$real_core_count
export STARPU_FXT_TRACE=1
export STARPU_GENERATE_TRACE=1

export OMP_NUM_THREADS=$real_core_count
export OMP_PLACES=cores
export OMP_PROC_BIND=true
export OMP_TOOL=enabled

export SCOREP_ENABLE_PROFILING=true
export SCOREP_ENABLE_TRACING=true
export SCOREP_VERBOSE=true
export SCOREP_TIMER=gettimeofday
# why 8G?
export SCOREP_TOTAL_MEMORY=8G

export KAAPI_RECORD_TRACE=1
export KAAPI_RECORD_MASK=compute,omp,perfctr
export KAAPI_PERF_EVENTS=TASK,TASKSPAWN
export KAAPI_TASKPERF_EVENTS=work,time
export KAAPI_DISPLAY_PERF=full

echo "Environment variables set up!"

# prepare our directory
mkdir $EXP_NAME
pushd $EXP_NAME

# copy the code folder
cp -r $EXP_DIR/code code
mkdir results

pushd code
make clean
make all LIBOMP_PATH="$LIBOMP_PATH" LIBOMP_OMPT_PATH="$LIBOMP_OMPT_PATH"
popd

# init the results csv
results_csv=results/${HOST}_data.csv
echo "node,matrix_size,block_size,runtime,compute_time,total_time" > $results_csv

# execute the experiment
while read -r runtime matrix num_blocks; do
    echo "-> Parameters set to: $runtime $matrix $num_blocks"
    run_id=${runtime}_${matrix}_${num_blocks}

    # output log file
    log_file=results/${run_id}.log

    LD_LIBRARY_PATH=$(readlink -f code/lib):$LAPACK_PATH/lib

    trace_dir=$PWD/results/$run_id
    mkdir -p $trace_dir

    if [[ $runtime = starpu ]] || [[ $runtime = kstar_starpu ]]; then
        export STARPU_FXT_PREFIX=$trace_dir/
        LD_LIBRARY_PATH+=:$HDF5_PATH/lib
        LD_LIBRARY_PATH+=:$STARPU_PATH/lib
    elif [[ $runtime = kstar_starpu ]]; then
        LD_LIBRARY_PATH+=:$KSTAR_PATH/lib
    elif [[ $runtime = ompt ]]; then
        LD_LIBRARY_PATH+=:$LIBOMP_OMPT_PATH/lib
    elif [[ $runtime = libkomp_gcc ]] || [[ $runtime = libkomp_clang ]]; then
        export LD_PRELOAD=$LIBKOMP_PATH/lib/trace-libomp.so
        LD_LIBRARY_PATH+=:$LIBKOMP_PATH/lib
    elif [[ $runtime = scorep ]]; then
        export SCOREP_EXPERIMENT_DIRECTORY=$trace_dir
    fi

    export LD_LIBRARY_PATH=$LD_LIBRARY_PATH

    # execute given runtime and log results
    timeout 2h ./code/build/block_qr_$runtime \
         $matrix \
         $num_blocks \
         $RAND_SEED \
         $MAXVAL > $log_file 2>&1

    # get compute and total times from output
    ctime=$(grep -w compute_time $log_file | awk '{print $2}')
    ttime=$(grep -w total_time $log_file | awk '{print $2}')

    case $runtime in
        ompt)
            mv events.out $trace_dir
            ;;
        libkomp_gcc|libkomp_clang)
            mv /tmp/events.*.evt $trace_dir
            pushd $trace_dir
            katracereader --csv events.*
            popd
            mv stat.* $trace_dir
            ;;
        starpu|kstar_starpu)
            mv activity.data dag.dot data.rec distrib.data paje.trace tasks.rec trace.html trace.rec $trace_dir
            ;;
    esac

    # add the execution data to the csv
    echo ${HOST},${matrix},${num_blocks},${runtime},${ctime},${ttime} >> $results_csv

    echo

    export LD_PRELOAD=
done < $EXP_DIR/runs.plan${PLAN_SUFFIX:-}

# gather node info
./code/scripts/node_info.sh > env.node

# create the data dir if it isn't already there
[ ! -d $EXP_DIR/data ] && mkdir $EXP_DIR/data

# zip everything and commit to EXP_DIR
tar czf $EXP_DIR/data/${EXP_NAME}_data.tar.gz *

popd
rm -rf $SCRATCH/*
#+end_src

*** Post-processing

Now we gotta do some post-processing in order to manipulate all collected data.

#+begin_src bash :exports both :results output
set -euo pipefail

DATA_DIR=experiments/exp02/data

for tar_file in $DATA_DIR/*.tar.gz; do
    echo "Extracting ${tar_file}..."
    mkdir ${tar_file%.tar.gz}
    tar xzf $tar_file -C ${tar_file%.tar.gz}
done

echo "Done extracting the files!"
#+end_src

#+RESULTS:
: Extracting experiments/exp04/data/exp04_cei1_48_data.tar.gz...
: Extracting experiments/exp04/data/exp04_cei2_48_data.tar.gz...
: Extracting experiments/exp04/data/exp04_cei3_48_data.tar.gz...
: Extracting experiments/exp04/data/exp04_cei4_48_data.tar.gz...
: Extracting experiments/exp04/data/exp04_cei5_48_data.tar.gz...
: Extracting experiments/exp04/data/exp04_cei6_48_data.tar.gz...
: Extracting experiments/exp04/data/exp04_cei7_48_data.tar.gz...
: Extracting experiments/exp04/data/exp04_cei8_48_data.tar.gz...
: Done extracting the files!

With all files extracted, lets collect the traces and transform them into
CSVs...

**** libomp -> csv

A simple R function using =tidyr= will work here:

#+begin_src R :session :results output :exports both
options(crayon.enabled = FALSE)
supressMessages(library(tidyverse))
supressMessages(library(functional))

libomp_csv <- function(trace_file, out_file) {
  trace_file %>%
    read_delim(delim = " ", col_names = FALSE, col_types = cols()) %>%
    rename(thread = X1,
           task = X2,
           callback = X3,
           operation = X4,
           time = X5) %>%
    write_csv(out_file)
}

data_dir <- "experiments/exp04/data"

data_dir %>%
  list.files(pattern = glob2rx("ompt*.trace"),
             recursive = TRUE, full.names = TRUE) %>%
  mapply(., FUN = libomp_csv, lapply(paste, ., ".csv", sep = ""))
#+end_src

**** libgomp -> csv

Here we'll need an extra script to do the conversion, called
=otf2ompprint2paje.pl=:

#+begin_src bash :exports both :results output
# attention!
# you need both scorep and pajeng installed in order to run this command
# also, be sure that both are in your path

cd experiments/exp04/data

for trace_file in $(find . -name 'scorep*.trace'); do
    ./scripts/otf2ompprint2paje.pl $trace_file |
        pj_dump --user-defined |
        grep ^State > ${trace_file}.csv
done
#+end_src

Then, lets put that generated CSV in an acceptable shape:

#+begin_src R :session :results output :exports both
options(crayon.enabled = FALSE)
supressMessages(library(tidyverse))
supressMessages(library(functional))

libgomp_csv <- function(trace_file, out_file) {
  trace_file %>%
    read_csv(col_names = FALSE, col_types = cols(), trim_ws = TRUE) %>%
    rename(worker = X2,
           start = X4,
           end = X5,
           duration = X6,
           imbrication = X7,
           value = X8,
           job.id = X9) %>%
    write_csv(out_file)
}

data_dir <- "experiments/exp04/data"

data_dir %>%
  list.files(pattern = glob2rx("scorep*.trace.csv"),
             recursive = TRUE, full.names = TRUE) %>%
  mapply(., FUN = libomp_csv, .)
#+end_src

**** starpu -> csv

Here, we'll use =starvz=! Given that =starvz= presumes the default file name for
StarPU traces, we'll do some symbolic links in order to analyze those traces.

#+begin_src bash :exports both :results output
set -euo pipefail

process_trace() {
    mkdir -p $2
    pushd $2
    ln -s $1 prof_file_${USER}_0
    # starvz
    # mv files that starvz created
    popd
    rm -rf $2
}

DATA_DIR=$(readlink -f experiments/exp04/data/)
TRACES_STARPU=$(find $DATA_DIR -type f -wholename '*/results/starpu/*.trace')

cd $DATA_DIR

i=0
for file in $TRACES_STARPU; do
    process_trace $file tmp_$((i++)) &
done

wait
#+end_src

*** Analysis
**** StarVZ - Starpu & Kstar
***** Get starvz scritps
 #+begin_src shell :results output :exports both :eval no-export
git clone --depth=1 git@github.com:schnorr/starvz.git starvz-src && cd starvz-src
git rev-parse HEAD
rm -rf .git
 #+end_src

 #+RESULTS:
 : cb2b7a233d895fda057f8561ec4e5a07f1dbb489

***** Get starpu and install it (needed by some starvz scripts)
 #+begin_src shell :results output :exports both :eval no-export
git clone --depth=1 http://gitlab+deploy-token-127235:BZMob8RJoRPZAdLtsstX@gitlab.com/viniciusvgp/customSpack.git starpu-spack && cd starpu-spack
git rev-parse HEAD
rm -rf .git

./install_spack.sh -smy
./src/spack/bin/spack install starpu@1.3.1+fxt+poti~examples~mpi+openmp
 #+end_src

 #+RESULTS:
 : 50afedcc9e7e061cdb1df610507ef8dbef83f756



***** Phase 1
 #+name: raw-simu-starvz
 #+header: :var rawPath="./experiments/exp02/results-draco1-wip"
 #+begin_src sh :results output :exports both :cache yes :eval no-export
export PATH=starvz-src/:$PATH
export PATH=~/misc/pajeng/bin:$PATH
export PATH=`starpu-spack/src/spack/bin/spack location -i starpu`/bin/:$PATH

for file in `find $rawPath -maxdepth 1 -type d -name "*starpu*"`;  do
    # starvz
    ./starvz-src/src/phase1-workflow.sh $file qr
done
 #+end_src


***** Loading StarVZ
 #+begin_src R :results output :exports both :session Rvz :eval no-export
library(starvz)
 #+end_src


***** Reading Feather
 #+header: :var rawDirExp="./experiments/exp02/results-draco1-wip"
 #+begin_src R :results output :exports both :session Rvz :eval no-export
blockQrStarpuKstar <-
    grep("starpu", list.dirs(path=rawDirExp, full.names=TRUE, recursive=FALSE), value=TRUE) %>%
    lapply(function(fl){
        print(paste0("Working on ", fl))
        the_fast_reader_function(fl)
    })

names(blockQrStarpuKstar) <-
    grep("starpu", list.dirs(path=rawDirExp, full.names=TRUE, recursive=FALSE), value=TRUE) %>%
    lapply(function(fl){
        #nm <- basename(fl) %>% strsplit("_") %>% unlist()
        #nm[0:(length(nm)-2)] %>% paste(collapse ='-')
        nm <- basename(fl) %>% str_replace_all("_", "-") 
    })
 #+end_src


***** Full config
#+name: getFullConfig
 #+begin_src shell :results output :exports both :eval no-export :cache yes
wget https://raw.githubusercontent.com/schnorr/starvz/master/full_config.yaml
 #+end_src

 #+RESULTS:

#+name: fullConfig
#+header: :var dep1=getFullConfig
 #+begin_src R :results output :exports both :session Rvz :eval no-export :cache yes
pajer <- config::get(file = "full_config.yaml")
 #+end_src

***** Custom config
#+name: customConfig
#+header: :var dep1=fullConfig
#+begin_src R :results output :session Rvz :exports both
pajer$st$outliers = FALSE
pajer$st$cpb = FALSE
pajer$pmtool$bounds$active = FALSE
pajer$activenodes$active = FALSE
pajer$computingnodes$active = FALSE
pajer$kiteration$active = FALSE
pajer$lackready$active=TRUE
pajer$lackready$aggregation = 500
pajer$st$alpha = 0.8
     #+end_src

 #+RESULTS:

***** Plots
#+header: :var dep1=customConfig
 #+begin_src R :results output graphics :file img/trace-qr-starpu-kstar-starvz.png :exports both :width 1350 :height 2000 :session Rvz :eval no-export

#state_chart(blockQrStarpuKstar[[1]])
the_master_function(blockQrStarpuKstar[[1]])
 #+end_src

 #+RESULTS:
 [[file:img/trace-qr-starpu-kstar-starvz.png]]

***** All Plots StarPU & Kstar
****** Generating ORG blocks                                       :noexport:
#+name: starpukstarnames
#+begin_src R :results output :session Rvz :exports none :eval no-export
blockQrStarpuKstar %>% names
#+end_src

#+RESULTS: starpukstarnames
: [1] "kstar-starpu" "starpu"

#+header: :var lista=starpukstarnames
#+begin_src shell :results value raw :exports results :eval no-export
echo " "
echo "****** Full View Chart"
for r in $lista; do
    echo "******* Execution $r"
    echo "#+header: :var dep1=customConfig"
    r2=$(echo $r | sed "s/\"//g")
    echo "#+begin_src R :results output graphics :file img/trace-qr-$r2-starvz.png :exports both :width 1400 :height 800 :session Rvz :eval no-export "
    echo "  the_master_function(blockQrStarpuKstar[[$r]])"
    echo "#+end_src"
    echo ""
done;
 #+end_src

****** Full View Chart
******* Execution "kstar-starpu"
  #+header: :var dep1=customConfig
  #+begin_src R :results output graphics :file img/trace-qr-kstar-starpu-starvz.png :exports both :width 1400 :height 800 :session Rvz :eval no-export
    the_master_function(blockQrStarpuKstar[["kstar-starpu"]])
  #+end_src

  #+RESULTS:
  [[file:img/trace-qr-kstar-starpu-starvz.png]]

******* Execution "starpu"
  #+header: :var dep1=customConfig
  #+begin_src R :results output graphics :file img/trace-qr-starpu-starvz.png :exports both :width 1400 :height 800 :session Rvz :eval no-export
    the_master_function(blockQrStarpuKstar[["starpu"]])
  #+end_src

  #+RESULTS:
  [[file:img/trace-qr-starpu-starvz.png]]



**** Libkomp - Clang & Gcc
***** Functions in R
 #+name: rLibkomp
 #+begin_src R :results output :exports both :session R
library(ggplot2)
library(dplyr)

# helper: convert s to the date
date<-function(d) { as.POSIXct(d, origin="1970-01-01"); }


readtrace <- function (filename) {
   df <- read.csv(filename, header=TRUE, sep=",", strip.white=TRUE);
   df <- df %>% filter((Explicit==1)) %>% as.data.frame();
   df$Start <- df$Start*1e-9; # Convert ns to second
   df$End <- df$End*1e-9;
   df$Duration <- df$Duration*1e-9;
   df;
}

gantt_libkomp <- function (dtx) {
     
    # time zero (start of  first task in the DAG)
    zero <- dtx %>% filter(Name == "dgeqrt") %>% .$Start %>% min
    dtx %>% mutate(Start=Start-zero, End=End-zero) -> dty

    dty %>% mutate(Resource=as.factor(Resource)) -> dta

    # End of last task
    makespan = dta %>% filter(Name == "dgeqrt") %>% .$End %>% max
    makespan_y = (dta %>% .$Resource %>% unique %>% length) / 2

    # blockQrLibkomp[[1]] %>% filter(Name == "dgeqrt") %>% summarize(Min=min(Start), Max=max(Start)) %>% mutate(Total=Max-Min) %>% mutate(Max=Max-Min) %>% mutate(Min=0)

    dti <- dta %>% group_by(Resource) %>% summarize(Idleness = (1-(sum(End-Start)/makespan))*100)
    
    ggplot(data=dta, aes(x=date(Start), y=Resource)) +
        geom_rect(aes(fill=Name,
                      xmin=date(Start),
                      xmax=date(End),
                      ymin=as.numeric(Resource)-.4,
                      ymax=as.numeric(Resource)+.4)) +
        annotate("text", x = date(makespan), y = makespan_y, label = round(makespan, 2), angle=90) +
        geom_label(data=dti, aes(x=date(makespan*(-0.05)), y=Resource, label=paste(round(Idleness, 2), "%"))) +
        xlab("Time") +
        ylab("Application Threads") +
        theme_bw() +
        theme (legend.position = "bottom") +
        scale_fill_manual(values = c("#9bb6dd", "#96e3a2", "#f68285", "#d194d0")) +
        scale_y_discrete()
}

 #+end_src


***** Reading Data
#+header: :var rawDirExp="./experiments/exp02/results-draco1-wip"
#+header: :var fdep=rLibkomp
#+begin_src R :results output :exports both :session R :eval no-export

blockQrLibkomp <-
    grep("libkomp", list.dirs(path=rawDirExp, full.names=TRUE, recursive=FALSE), value=TRUE) %>%
    lapply(function(fl){
        print(paste0("Working on ", fl))
        readtrace(paste(fl, "tasks.csv", sep="/"))
    })

names(blockQrLibkomp) <-
    grep("libkomp", list.dirs(path=rawDirExp, full.names=TRUE, recursive=FALSE), value=TRUE) %>%
    lapply(function(fl){
        #nm <- basename(fl) %>% strsplit("_") %>% unlist()
        #nm[0:(length(nm)-2)] %>% paste(collapse ='-')
        nm <- basename(fl) %>% str_replace_all("_", "-") 
    })

#+end_src

  #+RESULTS:
  : [1] "Working on ./experiments/exp02/results-draco1-wip/libkomp_clang_8192_256"
  : [1] "Working on ./experiments/exp02/results-draco1-wip/libkomp_gcc_8192_256"


We should recompile this with -g option to include at least filenames
and row numbers...

* Resources
:PROPERTIES:
:ATTACH_DIR: resources/
:END:

Home to anything I like or think is relevant to the task at hand.

** Archive

Home to old stuff

*** Deps script backup

#+begin_src bash :shebang "#!/bin/bash" :tangle experiments/exp01/deps.sh
#SBATCH --time=3:00:00
#SBATCH --chdir=.
#SBATCH --output=/home/users/hcpsilva/slurm_outputs/%x_%j.out
#SBATCH --error=/home/users/hcpsilva/slurm_outputs/%x_%j.err
#SBATCH --mail-type=END,FAIL
#SBATCH --mail-user=hcpsilva@inf.ufrgs.br

# more robust script
set -euo pipefail

INSTALL_DIR=$1/$SLURM_JOB_PARTITION
SPACK_DIR=${2:-$HOME/spack-erad}

pushd $HOME

if [ ! -d $SPACK_DIR ]; then
    echo "spack not yet installed!"
    git clone http://gitlab+deploy-token-127235:BZMob8RJoRPZAdLtsstX@gitlab.com/viniciusvgp/customSpack.git $SPACK_DIR
    pushd $SPACK_DIR
    ./install_spack.sh -symr
    popd
fi

. $SPACK_DIR/src/spack/share/spack/setup-env.sh

# find available compilers for this machine
spack compiler find

# get current node info
ARCH=$(spack arch)

# create the install dir if there isn't one
[ ! -d $INSTALL_DIR ] && mkdir -p $INSTALL_DIR

pushd $INSTALL_DIR

if [ ! -d openblas-0.3.7 ]; then
    echo "OpenBLAS not yet installed!"
    mkdir openblas-0.3.7
    spack install openblas@0.3.7 arch=$ARCH
    spack view -d true soft openblas-0.3.7 openblas@0.3.7 arch=$ARCH
fi

if [ ! -d hdf5-1.10.5 ]; then
    echo "HDF5 not yet installed!"
    mkdir hdf5-1.10.5
    spack install hdf5@1.10.5 arch=$ARCH
    spack view -d true soft hdf5-1.10.5 hdf5@1.10.5 arch=$ARCH
fi

if [ ! -d starpu-1.3.1 ]; then
    echo "StarPU not yet installed!"
    mkdir starpu-1.3.1
    spack install starpu@1.3.1~fxt~poti~examples~mpi+openmp arch=$ARCH
    spack view -d true soft starpu-1.3.1 starpu@1.3.1~fxt~poti~examples~mpi+openmp arch=$ARCH
fi

if [ ! -d netlib-lapack-3.8.0 ]; then
    echo "lapack not yet installed!"
    mkdir netlib-lapack-3.8.0
    spack install netlib-lapack@3.8.0 arch=$ARCH
    spack view -d true soft netlib-lapack-3.8.0 netlib-lapack@3.8.0 arch=$ARCH
fi

if [ ! -d libomp-6.0 ]; then
    echo "libomp not yet installed!"
    pip install --user lit
    mkdir libomp-6.0
    git clone https://github.com/llvm-mirror/openmp.git libomp-6.0/openmp
    pushd libomp-6.0/openmp
    git checkout release_60
    mkdir build
    pushd build
    LLVM_PATHS=$(find /usr/lib -name 'llvm-[0-9]*' | sed -e 's/$/\/bin/' | paste -s -d':' -)
    export PATH+=:$LLVM_PATHS
    cmake -DCMAKE_C_COMPILER=clang -DCMAKE_CXX_COMPILER=clang++ -DCMAKE_INSTALL_PREFIX=$INSTALL_DIR/libomp-6.0 -DLIBOMP_OMPT_SUPPORT=on -DLIBOMP_OMPT_OPTIONAL=on -DLIBOMP_STATS=on ..
    make -j
    make -j install
    popd
    popd
fi

if [ ! -d libkomp-master ]; then
    echo "libkomp not yet installed!"
    mkdir libkomp-master
    spack install --keep-stage libkomp@master+the+affinity+numa~tracing~papi+vardep arch=$ARCH
    spack view -d true soft libkomp-master libkomp@master+the+affinity+numa~tracing~papi+vardep arch=$ARCH
fi

if [ ! -d kstar-starpu-master ]; then
    echo "kstar not yet installed!"
    mkdir kstar-starpu-master
    spack install --keep-stage kstar@master+starpu^starpu@1.3.1~fxt~poti~examples~mpi+openmp arch=$ARCH
    spack view -d true soft kstar-starpu-master kstar@master+starpu^starpu@1.3.1~fxt~poti~examples~mpi+openmp arch=$ARCH
fi

popd
popd
#+end_src
